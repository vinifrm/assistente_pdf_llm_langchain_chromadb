{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Biblioteca padrão do Python usada para interagir com o sistema operacional, como manipular arquivos e diretórios.\n",
    "import openai  # Biblioteca para acessar a API da OpenAI, permitindo o uso de modelos de linguagem como GPT.\n",
    "import langchain  # Framework para desenvolvimento de aplicativos baseados em linguagem natural, integra várias ferramentas de IA.\n",
    "import chromadb  # Biblioteca para interagir com o ChromaDB, um sistema de armazenamento de embeddings vetoriais.\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader  # Carrega documentos PDF de um diretório para processamento.\n",
    "from langchain_community.embeddings import OpenAIEmbeddings  # Facilita a criação e o uso de embeddings gerados pelos modelos da OpenAI.\n",
    "from langchain_community.vectorstores import Chroma  # Interface para armazenar e recuperar embeddings no ChromaDB.\n",
    "from langchain_community.llms import OpenAI  # Permite integrar e usar modelos de linguagem da OpenAI em cadeias de processamento de linguagem.\n",
    "from langchain.chains.question_answering import load_qa_chain  # Carrega cadeias de processamento configuradas para question answering.\n",
    "import warnings  # Biblioteca padrão do Python para gerenciar avisos.\n",
    "warnings.filterwarnings('ignore')  # Configura a biblioteca de avisos para ignorar todos os avisos emitidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo texto de documento PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracao_pdf(path):\n",
    "\n",
    "    # instância da classe para leitura ds PDFs contidos na path\n",
    "    leitor = PyPDFDirectoryLoader(path)\n",
    "\n",
    "    # fazendo a leitura\n",
    "    # Cada documento contém o texto e, possivelmente, metadados associados. Estes documentos são convertidos em vetores (embeddings) usando um modelo de \n",
    "    # embeddings e armazenados no índice\n",
    "    documentos_lidos = leitor.load()\n",
    "\n",
    "    return documentos_lidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = r'C:\\Users\\Vinicius Moraes\\Google Drive\\estudos\\data science, machine learning, estatistica\\DSA\\ia generativa e llms\\mod05\\87-Cap05\\arquivos2'\n",
    "texto_artigo = extracao_pdf(caminho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texto_artigo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando gerador de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = 'API KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gerador_embeddings = OpenAIEmbeddings(api_key=os.environ['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "A função embed_query é utilizada para gerar embeddings de uma consulta de texto usando os modelos de embeddings da OpenAI. \n",
    "Esses embeddings podem ser usados para fins de busca, recuperação de informações, e outras aplicações de NLP \n",
    "'''\n",
    "vetores = gerador_embeddings.embed_query(\"Me fale sobre o Flamengo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.015223479866010815,\n",
       " -0.008045822893365985,\n",
       " -0.0068536228199858225,\n",
       " -0.01039965423731675,\n",
       " -0.007672878469947157]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vetores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo o Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criando o nome do indice\n",
    "# index_nome = 'artigo'\n",
    "index_nome = {\"name\": \"artigo\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 0}, page_content='Preprint. A version of this work is under review.\\nFrom Words to Numbers: Your Large Language Model Is Se-\\ncretly A Capable Regressor When Given In-Context Examples\\nRobert Vacareanu∗\\nUniversity of Arizona\\nTechnical University of Cluj-NapocaVlad-Andrei Negru\\nTechnical University of Cluj-Napoca\\nVasile Suciu\\nTechnical University of Cluj-NapocaMihai Surdeanu\\nUniversity of Arizona\\nAbstract\\nWe analyze how well pre-trained large language models (e.g., Llama2,\\nGPT-4, Claude 3, etc) can do linear and non-linear regression when given\\nin-context examples, without any additional training or gradient updates.\\nOur findings reveal that several large language models (e.g., GPT-4, Claude\\n3) are able to perform regression tasks with a performance rivaling (or even\\noutperforming) that of traditional supervised methods such as Random\\nForest, Bagging, or Gradient Boosting. For example, on the challenging\\nFriedman #2 regression dataset, Claude 3 outperforms many supervised\\nmethods such as AdaBoost, SVM, Random Forest, KNN, or Gradient Boost-\\ning. We then investigate how well the performance of large language\\nmodels scales with the number of in-context exemplars. We borrow from\\nthe notion of regret from online learning and empirically show that LLMs\\nare capable of obtaining a sub-linear regret.1\\n1 Introduction\\nFigure 1: Mean Absolute Error ( ↓) compar-\\nison between three large language mod-\\nels (LLMs) and four traditional supervised\\nmethods for learning a linear regression\\nfunction with one informative variable out\\nof two. Given only in-context examples\\nand without any additional training or gra-\\ndient updates, pre-trained LLMs such as\\nClaude 3, GPT-4, or DBRX can outperform\\nsupervised methods such as Random For-\\nest or Gradient Boosting.Large Language Models (LLMs) are capable of\\nlearning to perform a task when given exam-\\nples of that task in their context, without any\\nadditional training. This surprising capabil-\\nity, called in-context learning (ICL) Brown et al.\\n(2020), emerges just from next-token prediction\\nfor sufficiently large models.\\nWe use regression tasks to analyze the in-\\ncontext capabilities of already pre-trained large\\nlanguage models (LLMs), such as Llama2, GPT-\\n4, or Claude 3. Garg et al. (2022) have pre-\\nviously explored the range of functions that\\ntransformers, when trained specifically for in-\\ncontext learning, are capable of learning. How-\\never, contemporary LLMs emerge as capable\\nin-context learners without being specifically\\ntrained for it. We extend previous work and\\nanalyze the extent to which LLMs, decoder-\\nonly transformers trained auto-regressively for\\nnext-token prediction, are capable of learning\\nregression functions when given in-context ex-\\nemplars, without any additional form of su-\\npervision or training .\\n∗Correspondence to rvacareanu@arizona.edu\\n1Code available at https://github.com/robertvacareanu/llm4regression\\n1arXiv:2404.07544v2  [cs.CL]  30 Apr 2024'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 1}, page_content='Preprint. A version of this work is under review.\\nSimilar to previous work Garg et al. (2022), we use (synthetic) regression datasets. Synthetic\\nregression datasets have the following advantages:\\n(i) Algorithmically generated : The data is guaranteed to be generated deterministically, by\\na well-determined (and logical) formula. This property makes them suitable to use when\\ninvestigating whether a given model is capable of unraveling the underlying structure of\\nthe data.\\n(ii) Difficulty control : The user has direct access to the difficulty of the synthetic regression\\nproblem and can investigate the cases of simple linear regressions of the form y=ax+b,\\nto more difficult problems such as Friedman #2, a highly non-linear function used for\\nbenchmarking:\\ny=s\\n(x2\\n1+ (x2·x3−1\\nx2·x4)2)\\n(iii) Data availability : Lastly, synthetic datasets present the advantage of allowing the user\\nto generate novel data in large(r) quantities. Additionally, it ensures that models are less\\nlikely to have been previously exposed to these specific problems.\\nFormally, let Dnbe a dataset consisting of ninput-output examples: Dn=\\n{(x1,y1),. . .,(xn,yn)}, where xi∈Rd, with 1 ≤d≤20 typically. We have yi=f(xi),\\nwhere f:Rd→Rand yi∈R. We do not put any restrictions on fand study functions\\nranging from simple linear predictions (i.e., f(x) = ax+b) to more complex and highly\\nnon-linear functions (e.g., f(x) =10x+sin(5πx) +cos(6πx)). We study how well various\\nmodels such as LLMs (e.g., GPT-4), traditional supervised models (e.g., Random Forest),\\nand unsupervised baselines (e.g., random prediction) are capable of predicting yn+1when\\ngiven access to ninput-output examples (i.e., Dn) and xn+1.\\nOur study shows that pre-trained large language models (LLMs) display a surprisingly\\ngood performance on various regression tasks. For example, in Figure 1, without any\\nparameter update , Claude 3 approaches the performance of a Linear Regression model and\\nlargely outperforms other supervised methods such as Random Forest or Gradient Boosting\\non a randomly generated linear regression dataset, with one informative variable out of\\ntwo.\\n2 Experimental Setup\\nWe describe the models and the datasets we use in our experiments below.\\n2.1 Datasets\\nWe experiment with 3 types of datasets: (1) linear regression, (2) non-linear regression, and\\n(3) regression datasets with non-numerical inputs. We describe each below.\\n2.1.1 Linear Regression Datasets\\nWe experiment with linear regression tasks of the form y=wx+b, where w,x∈Rd,b∈R,\\nand y∈R, with 1 ≤d≤20. We vary both d, the dimension of the input x, and the number\\nofinformative variables (i.e., the number of non-zero elements in w).\\nWhen generating a dataset, we sample the input xfromN(0, 1). We sample the weight\\nvector wfrom Uni f orm (0, 100 ).2\\n2.1.2 Non-Linear Regression Datasets\\nFor non-linear regression problems, we use the three problems introduced by Friedman,\\ncalled Friedman #1 ,Friedman #2 , and Friedman #3 (Friedman, 1991; Breiman, 1996). For\\n2We used sklearn. Please see make regression for more details.\\n2'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 2}, page_content='Preprint. A version of this work is under review.\\nexample, Friedman #1 is defined as follows:\\ny(x) =10∗sin(x0x1π) +20(x2−0.5)2+10x3+5x4+ϵ∗N(0, 1)\\nWhere y∈R,x∼Uni f orm (0, 1)dand ϵ∈R. We have 5 ≤d. When d>5, the extra\\ndimensions are ignored for the computation of y.\\nWhile we create these datasets with different random seeds, resulting in different Dn,\\nmaking a particular Dnvery unlikely to have been seen by the LLMs during training, it is\\nstillpossible that they have seen different Dnoriginating from the same generator function f.\\nIn an attempt to mitigate this risk, we created 4 new non-linear datasets. We describe them in\\nthe Appendix B. For example, one of these functions is: y(x) =10x+sin(5πx) +cos(6πx),\\nwhere x∼Uni f orm (0, 1)\\nTo supplement the non-linear regression datasets and following Garg et al. (2022), we create\\ndatasets using randomly initialized neural networks. We explore the outputs of 2 types of\\nneural networks: (1) a sequence of simple linear layers with ReLU non-linearity in-between,\\nand (2) the output of a randomly initialized transformer encoder block.\\n2.1.3 Regression With Non-Numerical Inputs\\nTo further investigate whether the models are able to learn abstract tasks beyond those\\nsubsumed by numerical regression Razeghi et al. (2022), we design the following tasks.\\nWe (randomly) map symbols (i.e., characters) to numbers (e.g., a→1). We then randomly\\nsample a subset of these symbols in order to keep the context size manageable and to not\\nneed a large number of examples. We map the symbols to a numerical value by sampling a\\nweight vector w∈Rdand doing a dot product between it and the corresponding values of\\neach symbol. We use lowercase ASCII letters as our symbols (i.e., a . . .z). We randomly\\nsample 5 symbols which will serve as our vocabulary. We include the pseudocode in\\nAppendix B.11.\\n2.2 Models\\nWe experiment with three types of models: (1) large language models such as GPT-4, (2)\\nsupervised models such as Random Forest, and (3) heuristic-based unsupervised models\\nsuch as random sampling. All models have access to the same train data and are evaluated\\non the same test partition. They have access to an input dataset Dnand are asked to predict\\ntheyn+1corresponding to the xn+1. The train partition is used for in-context exemplars for\\nLLMs and supervised training for supervised methods. Due to budget constraints and the\\ncontext size limitations of the LLMs, we round input values to two decimal places.3We\\nrepeat each experiment with different random seeds.\\nLLMs: We use a total of 12 large language models (LLMs), both open and private. Specifi-\\ncally, we use Mistral7B, Mixtral8x7B, CodeLlama70B, Llama2 70B, Yi 34B, DBRX ( weights\\navailable ) and ChatGPT, GPT-4 (OpenAI) and Claude 3 Opus, Claude 3 Sonnet (Anthropic),\\nGemini Pro (Google), and Mistral Medium (Mistral) ( weights not available ). The models we\\nuse cover a wide range of parameters, from 7B or less (Mistral) to 132B or more (DBRX).4Un-\\nless otherwise specified, we interact with the models only through prompting and in-context\\nexemplars. We use the same prompt for all models and do not do any prompt tuning. The\\nprompt is of the form Feature 1: <number>\\\\nFeature 2: <number>\\\\nOutput: <number> .\\nIn-context exemplars are separated with two new lines “ \\\\n\\\\n ”. For the test example, the\\nmodel is asked to predict the number corresponding to the Output variable. We observed\\nthat some models tend to provide additional explanations, before outputting the final num-\\nber. To prevent this behavior, we add an additional text in the beginning, instructing the\\n3We provide extra experiments without rounding in Appendix L to show that the strong results we\\nobserved are not an artifact of rounding.\\n4Since the number of parameters for some models is not disclosed, it is possible that certain closed\\nmodels may have fewer than 7B or more than 132B parameters.\\n3'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 3}, page_content='Preprint. A version of this work is under review.\\n(a) Informative Variables: 1; Total Variables: 3\\n (b) Informative Variables: 2; Total Variables: 2\\nFigure 2: The performance, as measured by the Mean Absolute Error ( ↓), across large\\nlanguage models (LLM), traditional supervised models and unsupervised models on two\\ndifferent random regression tasks: (a) sparse linear regression, where only 1 out of a total\\nof 3 variables is informative, and (b) linear regression with two informative variables. The\\nresults are averages with 95% confidence intervals from 100 runs with varied random seeds.\\nAll LLMs perform better than the unsupervised models, suggesting a more sophisticated\\nunderlying mechanism at play in ICL. Furthermore, some LLMs (e.g., Claude 3) even\\noutperform traditional supervised methods such as Random Forest or Gradient Boosting.\\nLLM to only output the number. We give a complete example in Appendix C.1.1. Addition-\\nally, we analyze the explanations provided by the models in Appendix J, finding that there\\nis sometimes a discrepancy between the rationale given for their predictions and the actual\\npredicted values Unless otherwise specified, we use a temperature of 0.\\nSupervised Baselines: We use a total of 10 traditional supervised models, available in\\nmost statistical learning packages. We use: Linear Regression (4 versions: no regularization,\\nRidge regularization, and Lasso Regularization, no regularization and with polynomial\\nfeatures), Multi-Layer Perceptron (6 versions, 3 versions with different widths (Hornik et al.,\\n1989) and 3 versions with different depths), Random Forest, Bagging, Gradient Boosting,\\nAdaBoost, SVM, KNN, Kernel Ridge, and Splines. Similar to the LLM case, we do not tune\\nany hyperparameters and use the defaults available in sklearn. It is important to note that\\nthese supervised baselines are very strong: (1) many of them are the results of algorithms\\nspecifically designed for regression (e.g., Splines); (2) all perform parameter updates (unlike\\nan LLM with ICL); and (3) the default hyperparameters, as set in widely-used statistical\\npackages, have been refined over time to offer reliable and generally strong performance\\nacross a variety of scenarios.\\nUnsupervised Baselines: In order to contextualize the performance of the LLMs and to\\nevaluate their effectiveness relative to basic heuristics, we incorporated the following series\\nof heuristic-based unsupervised baseline:\\n1.Average: Predicts the next value, yn+1, as the mean of all preceding outcomes:\\nyn+1=1\\nn∑n\\ni=1yi.\\n2.Last: Uses the most recent tuple (xn,yn)for prediction, such that yn+1=yn.\\n3.Random: Predicts yn+1by randomly selecting from the set of prior observations\\n{y1, . . . , yn}. The final prediction is thus yn+1=sample ([y1, . . . , yn])\\nAdditional details on the models are provided in Appendix C. We include results with addi-\\ntional models, such as the latest release of GPT-4 ( gpt-4-2024-04-09 ) or Mixtral Mixture of\\nExperts 8x22B in the Appendix F, where we present the average rank obtain by each model.\\n3 Large Language Models Can Do Linear Regression\\nOur first experiment intends to capture how well LLMs can do linear regression when given\\nonly in-context examples. To this end, we experiment with 4 different types of regression\\nproblems, varying the number of total variables and the number of informative variables.\\nWe provide a total of 50 tuples D50={(x,y)i|i=1. . .50}as in-context exemplars and\\n4'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 4}, page_content='Preprint. A version of this work is under review.\\nFigure 3: The rank of each method investigated over all four linear regression datasets.\\nRankings are visually encoded with a color gradient, where green means better performance\\n(higher ranks) and red indicates worse performance (lower ranks). Notably, very strong\\nLLMs such as Claude 3 and GPT-4 consistently outperform traditional supervised methods\\nsuch as Gradient Boosting, Random Forest, or KNN. (best viewed in color)\\nask the model to generate y51, corresponding to x51. We repeat the experiments with 100\\ndifferent random seeds and present the average and 95% confidence intervals.\\nWe present two bar plots in Figure 2, corresponding to two different datasets: (1)a dataset\\nconsisting of three variables, with a single informative variable ( Regression NI 1/3 ), and\\n(2)one dataset containing two random variables, where both variables are informative\\n(Regression NI 2/2 ). For LLMs, we selected Claude 3 Opus (Claude 3), GPT-4, and Gem-\\nini Pro, as they are the flagship closed-source models currently available, and Mixtral8x7B\\n(Mixtral), Llama2 70B (Llama 2), Yi 34B (Yi) and DBRX (DBRX) as the flagship open-weights\\nmodels. Traditional supervised models in our analysis included Linear Regression (LR),\\nMulti-Layer Perceptron (MLP), Random Forests (RF), and Gradient Boosting (GB). Addi-\\ntionally, we include a fifth supervised method, the one resulting in the best performance.5\\nWe would like to remark that this is a very strong baseline, as it highlights the best perfor-\\nmance obtainable with hindsight information. For the unsupervised baselines we included\\n(i) Average, and (ii) Random Sampling. We draw the following observations from this\\nexperiment:\\nFirst, LLMs, when given in-context examples of input-output pairs, exhibit a (perhaps\\nsurprisingly) good overall performance. When compared with unsupervised baselines, the\\nlarge language models always outperform them, indicating that the underlying mechanism\\nat play is more sophisticated than such simple heuristics.\\nSecond, we remark that LLMs in some cases outperform even supervised methods. For\\nexample, for the regression task with one informative variable out of a total of 3 ( Regression\\nNI 1/3 ), Claude 3 ranks 3 out of a total number of 31 models, only (slightly) behind Linear\\nRegression and Linear Regression + Poly . For example, when averaging the mean\\nabsolute error across all runs, Claude 3 obtains 0.14, while Linear Regression obtains\\n0.12. It largely outperforms other supervised methods such as Random Forest orGradient\\nBoosting , even though it no gradient updates were performed, nor it was specifically\\ndesigned for linear regression.6\\nLastly, we remark that this strong performance is not only specific to the current closed-\\nsource flagship models. For example, Mixtral outperforms supervised methods such as\\nRandom Forest or Gradient Boosting on the Regression NI 2/2 dataset.\\nAlongside the two bar plots, we include a heatmap in Figure 3 to show how each model\\nranks across different datasets. We show the datasets vertically and the models horizontally.\\n5If this method coincides with one previously selected, the subsequent best performer is chosen.\\n6Comparatively, Random Forest obtains 5.32, Gradient Boosting obtains 2.58, and GPT-4 obtains\\n2.26.\\n5'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 5}, page_content='Preprint. A version of this work is under review.\\nFor instance, Claude 3 Opus achieves the top rank ( rank=1 ) on the NI 1/1 dataset. Notably,\\nClaude 3 Opus and GPT-4 consistently perform better than methods such as AdaBoost,\\nGradient Boosting, KNN, or Random Forest. Out of the LLMs with open-weights, both\\nMixtral 8x7B and Yi 34B Chat outperform methods such as KNN or SVM on all four datasets.\\n(a) Friedman # 1\\n(b) Friedman # 2\\n(c) Friedman # 3\\nFigure 4: The performance of large language models (LLM), traditional supervised models\\nand unsupervised models on Friedman #1, #2, and #3. The results represent the averages\\nwith 95% confidence intervals over 100 different runs.\\nOverall, these results reveal that large language models, whether closed-source (e.g., Claude\\n3, GPT-4) or open-weights (e.g., DBRX, Mixtral 8x7B), are capable of performing linear re-\\ngression tasks using in-context exemplars composed of (x,y)pairs, all without the necessity\\nfor gradient updates. While the performance across these models varies, it consistently\\noutperforms that of unsupervised baselines, suggesting that the underlying mechanism\\nat play is more sophisticated than these simple heuristics. Moreover, specific LLMs (e.g.,\\nClaude 3, GPT-4) consistently exceed the performance of strong supervised baselines such\\nas Random Forests, Gradient Boosting, or KNN.\\n6'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 6}, page_content='Preprint. A version of this work is under review.\\nWe present extended results, encompassing a wider array of models and datasets, in Ap-\\npendix D.\\n4 Large Language Models Can Do Non-Linear Regression\\nWe extend our previous analysis to non-linear regression problems.\\n4.1 Friedman Benchmarks\\nWe use the 3 synthetic regression benchmarks introduced by Friedman (1991). Below, we\\nprovide the definition of the Friedman #2 dataset, with complete definitions for all datasets\\navailable in Appendix B.\\nf(x) =s\\nx2\\n1+\\x12\\nx2·x3−1\\nx2·x4\\x132\\n+ϵ· N(0, 1) (1)\\nwhere ϵrepresents noise added to the system, modeled as a Gaussian distribution N(0, 1),\\nand the variables x1,x2,x3, and x4are drawn from uniform distributions as follows:\\nx1∼ U(0, 100 ),x2∼ U(40π, 560π),x3∼ U(0, 1), and x4∼ U(1, 11).\\nOur findings for the Friedman #1, #2, and #3 benchmarks are presented in Figure 4. The\\nselection of methods follows to the same procedure used in Section 3: three leading closed-\\nsource LLMs, four leading open-weights LLMs, and five conventional supervised models–\\nincluding the best performing model–and two unsupervised baselines. We remark that the\\nstrong performance of LLMs persists in the non-linear case as well. For example, Claude\\n3 outperforms all but the Linear Regression with Polynomial Features ( LR + Poly ) on\\nFriedman #2.\\n4.2 New Regression Datasets\\nFigure 5: An example of one of our new non-linear\\nregression functions. The function was designed to\\nmimic a linear trend with oscillations.In an effort to mitigate the poten-\\ntial familiarity of models with pre-\\nexisting datasets encountered during\\ntheir pre-training phase, we exper-\\niment with two new non-linear re-\\ngression datasets which are unlikely\\nto have been part of the pre-training\\nphase. Our methodology is as fol-\\nlows. Our first novel dataset (called\\nOriginal #1 ), plotted in Figure 5, is\\ncreated to resemble a line with oscil-\\nlations:\\ny=10x+sin(5πx) +cos(6πx)(2)\\nWhere x∼ U(0, 100 ).\\nFor the next dataset (called Original\\n#2), we draw inspiration from the\\ndatasets introduced by Friedman,\\nbut we modify the domain of xand change the operands (e.g.,2→4). We provide an\\nexample below:\\ny= (x4\\n1+ (x2·x3−2√x2·√x4)2)3\\n4 (3)\\nIt is important to underscore that the primary goal of these novel datasets is not to construct\\ninherently difficult challenges for the LLMs, but rather to minimize the probability of\\n7'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 7}, page_content='Preprint. A version of this work is under review.\\nevaluating them on datasets they could have already seen during their training phase. We\\nprovide additional details on these datasets in Appendix B, along with additional datasets.\\nFor an in-depth analysis of potential data contamination concerns, including additional\\nexperiments conducted to address these issues, refer to Appendix N.\\n4.3 Discussion\\nWe summarize all our results in the form of a heatmap in Figure 6. For each dataset, we\\nrecord the relative rank of each method with respect to all the others. For example, Claude\\n3 Opus performs the best on Original 1 (rank=1 ). We structure our results in 3 blocks: (1)\\nLLMs (left), (2) Traditional Supervised Methods (middle), and (3) Unsupervised Methods\\n(right). We make the following observations:\\nFirst, on Original 1 (see Figure 5), LLMs largely outperform traditional supervised meth-\\nods. Remarkably, eight out of the ten highest-ranking methods in this context are LLMs.\\nThis strong performance on this dataset is exhibited by both private and open-weights\\nmodels. For example, DBRX outperforms all traditional supervised methods, despite no\\ngradient update.\\nSecond, we remark that the LLMs show a strong performance on all datasets introduced by\\nFriedman ( Friedman #1 ,Friedman #2 ,Friedman #3 ) and on all datasets introduced by us\\n(Original #1 ,Original #2 ).\\nOverall, our results show that LLMs with ICL are capable of performing non-linear\\nregression . For example, Claude 3 Opus outperforms Gradient Boosting and KNN on all 5\\ndatasets. We present extended results, encompassing a wider array of models and datasets,\\nin Appendix E. We observed that LLMs struggle on the datasets generated with randomly\\ninitialized neural networks (e.g., Simple NN #1 ,Transformer #1 ), although they remain,\\ngenerally, better than the unsupervised methods.\\nDue to space constraints, we included in Appendix K an analysis of the performance of\\nLLMs on non-numerical regression datasets. We found that even in this regime, LLMs\\noutperform the unsupervised baselines.\\nFigure 6: Rank of each model investigated on the non-linear regression datasets. LLMs\\nare capable of non-linear regression. For example, for Original #1 , eight out of the ten\\nhighest-ranking methods are LLMs. (best viewed in color)\\n8'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 8}, page_content='Preprint. A version of this work is under review.\\n(a) GPT-4 on Friedman #2\\n (b) Claude 3 on Original #1\\nFigure 7: The cumulative regret of two large language models on two different non-linear\\nregression dataset. Both show a sub-linear regret grow, indicating that as more data points\\nare observed, the models become increasingly efficient at predicting outcomes closer to the\\noptimal strategy derived in hindsight.\\n5 How Fast Do Large Language Models Adapt?\\nFollowing the surprising results that LLMs are capable of doing regression, when given the\\ntraining data in their context in the form of in-context exemplars, we investigate next how\\ntheir predictions improve with the number of examples given. Specifically, we empirically\\nanalyze whether the performance of the LLMs approaches that of the best possible fixed\\nstrategy over time.\\nBorrowing from the Online Learning community, we empirically analyze how the cumula-\\ntive regret (i.e., cumulative loss) grows with respect to the time step (number of examples\\nin context) Orabona (2019). Ideally, a good model should, over time, approach the quality\\nof decisions that the best fixed strategy, informed by hindsight, would have made. In\\nother words, the cumulative regret should, ideally, grow sub-linearly over time. To empir-\\nically estimate how the regret grows, we fit 3 curves: (1) Linear Fit: a∗x+b, (2) Sqrt Fit:\\na∗sqrt(x) +band (3) Log fit: a∗log(x) +b.7We then use the R2coefficient to determine\\nwhich curve fit is better. We show two qualitative plots in Figure 7. We summarize the\\nresults in Table 1, recording the curve fit with the highest R2coefficient for each model.\\nSince simply picking the best curve fit according to the R2score might tell an incomplete\\nstory, we include additional plots in Appendix G, covering multiple models and all seven\\ndatasets. We draw the following observations. First, the performance of large language\\nmodels improves with the number of examples, suggesting the mechanism at play is capable\\nofeffectively leveraging more data. Second, we remark that very capable LLMs, such as\\nClaude 3 or GPT-4 can obtain sub-linear regret, meaning that the predictions made by the\\nLLM approach the quality of decisions that the best algorithm would have made, leading to\\nnear-optimal performance in the long run.\\nWe remark that there are differences between our empirical analysis and online learning.\\nFirstly, while online learning often focuses on establishing theoretical regret bounds, our\\napproach is empirical, we only empirically show that the regret of certain LLMs grow\\nsub-linearly by using curve fitting and R2. To address potential concerns of overfitting\\nand enhance the robustness of our empirical findings, we repeated the experiment 3 times\\nand averaged the cumulative regret. Second, our results are only for finite (and relatively\\nsmall) time steps, diverging from the online learning norm of analyzing behavior as T\\napproaches infinity. To provide further evidence that the results are not an artifact of small\\nT, we performed the following experiment. We used GPT-4 and recorded its performance\\nacross multiple training dataset sizes, ranging from 20 to 500. We have observed that\\nthe performance of GPT-4 continues to improve as the number of in-context exemplars\\nincreases, suggesting that, our results are not an artifact of limited time steps. We include\\nthe associated plots in Appendix O.\\n7The choice of linear, square root, and logarithmic fits is motivated by their common appearance in\\ntheoretical regret bounds within the online learning literature.\\n9'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 9}, page_content='Preprint. A version of this work is under review.\\nModel \\\\Dataset Friedman 1 Friedman 2 Friedman 3 Original 1 Original 2 Regression NI 1/3 Regression NI 2/2\\nClaude 3 Opus linear sqrt sqrt log sqrt log log\\nGPT-4 linear sqrt sqrt log sqrt log sqrt\\nGemini Pro linear sqrt linear log sqrt sqrt sqrt\\nYi 34B Chat linear sqrt linear sqrt sqrt sqrt sqrt\\nMixtral 8x7B linear linear linear sqrt linear linear sqrt\\nMistral 7B linear linear linear sqrt linear linear linear\\nDBRX linear log linear log sqrt sqrt sqrt\\nAdaBoost linear sqrt linear sqrt sqrt sqrt sqrt\\nGradient Boosting sqrt sqrt linear log sqrt log sqrt\\nLinear Regression linear linear linear linear linear log log\\nLinear Regression + Poly sqrt log log linear log log log\\nRandom Forest linear sqrt linear sqrt sqrt sqrt linear\\nKNN linear linear linear log linear sqrt sqrt\\nTable 1: We show which curve-fit obtained the highest R2for multiple models and datasets.\\nThe slower the growth of the function, the better (i.e., log>sqrt>linear ).(best viewed in color)\\nFollowing the empirical evidence that LLMs are very capable regressors, despite not being\\ntrained for it, we hypothesize that (very capable) LLMs emerge from their training as very\\ngood online meta-learners (Finn et al., 2019; Mirchandani et al., 2023).\\n6 Related Work\\nThe in-context learning capability of large language models has garnered significant at-\\ntention Brown et al. (2020). How this capability emerges during a standard next-token\\nprediction pretraining and how it operates is still up for debate. A substantial body of\\nresearch is dedicated to exploring the parallels between in-context learning mechanisms\\nand traditional algorithms like gradient descent (Aky ¨urek et al., 2023; von Oswald et al.,\\n2022; Dai et al., 2023; Ahn et al., 2023; Cheng et al., 2023; Mahankali et al., 2024; Vladymyrov\\net al., 2024). For example, Aky ¨urek et al. (2023) and von Oswald et al. (2022) prove that\\ntransformers could theoretically implement gradient descent. Bai et al. (2023) shows that\\nthe transformer architecture can implement more complex in-context learning procedures,\\ninvolving algorithm selection. Cheng et al. (2023) argue that non-linear transformers learn\\nto implement gradient descent in function spaces. von Oswald et al. (2023) suggests that\\nperformance of transformer-based models may be due to an architectural bias towards\\nmesa-optimizaiton. Nonetheless, the extent to which pre-trained transformers actually\\nimplement gradient descent when given in-context examples remains a topic of debate\\n(Natan et al., 2023; Shen et al., 2023).\\nOther lines of work investigate the convergence of in-context learning (Wies et al., 2023;\\nHuang et al., 2023). Li et al. (2024) analyzes the training dynamics of transformers with\\nnonlinear attention and nonlinear MLP , expanding upon previous work which considered\\nsimpler transformer-based architectures (Huang et al., 2023; Tian et al., 2023). However, for\\nnatural language tasks such as sentiment analysis, it is unclear how much learning occurs\\nwith in-context examples (Min et al., 2022; Pan et al., 2023; Kossen et al., 2024). For example,\\nMin et al. (2022) shows that GPT-3 retains a strong performance even when the labels of\\nthe in-context exemplars are random. On the other hand, recent work (Hendel et al., 2023;\\nLiu et al., 2023) investigated how in-context learning creates task vectors, which can then be\\napplied to produce the output.\\nAnother question investigated in recent work is where does the in-context learning (ICL)\\nemerges from (Chan et al., 2022; Xie et al., 2022; Han et al., 2023). For example, Chan\\net al. (2022) shows that in-context learning appears when the training data has particular\\nproperties. Xie et al. (2022) analyzes in-context learning through a small scale synthetic\\ndataset (GINC). Han et al. (2023) identified a subset of the pre-training data that supports in-\\ncontext learning, showing how continuing pretraining on this subset increases the model’s\\nICL abilities.\\nAnother line of research, which is close to our work, is that of investigating what types\\nof “functions” can be learned through in-context learning Garg et al. (2022); Zhang et al.\\n(2023); Xing et al. (2024). Notably, all these works do not use pre-trained LLMs, but specif-\\nically train a transformer for the task. Garg et al. (2022) shows empirically that standard\\n10'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 10}, page_content='Preprint. A version of this work is under review.\\ntransformers can be trained from scratch to perform in-context learning of linear functions.\\nGuo et al. (2024) investigates more complex function classes. Wei et al. (2023) shows that\\nlarger language models are able to overcome their semantic priors when shown input-label\\nmappings. Zhang et al. (2023) train transformers with a single linear self-attention layer to\\nin-context learn linear regression tasks, showing that transformers are capable of obtaining\\na performance competitive with the best linear predictor. Bhattamishra et al. (2024) experi-\\nment with training various models to in-context learn boolean functions. Although not the\\nmain focus of their work, they also experiment with pre-trained models such as Llama 2\\nand GPT-4, showing that they obtain a performance similar to nearest-neighbor baselines\\nfor boolean functions.\\nDifferent from previous work, we investigate how pre-trained models, such as GPT-4 or\\nClaude 3, without any gradient updates , can learn various linear and non-linear function\\nclasses when given examples in-context and thoroughly compare them against multiple\\ntraditional supervised methods (Ruppert, 2004) such as Gradient Boosting (Schapire, 1989;\\nFriedman, 2001) or Random Forests (Breiman, 2001).\\n7 Conclusion\\nIn this paper, we examined the extent to which large language models such as Claude 3,\\nGPT-4, or DBRX are capable of performing the task of regression, when given input-output\\npairs as in-context examples, without any gradient updates.\\nWe showed that large language models are capable of doing both linear and non-linear\\nregression, with performance rivaling that of supervised methods such as Linear Regression\\nor Gradient Boosting. We then analyzed how their performance approaches that of the\\nbest possible fixed strategy as the number of in-context examples grows, showing how\\nvery capable models such as Claude 3 Opus or GPT-4 are capable of approaching the\\nquality of decisions that the best algorithm in hindsight would have made. Our results\\ndemonstrate that large language models are capable of doing regression when given in-\\ncontext examples of (input, output) pairs, despite not being explicitly trained to do so. We\\nleave the exploration of augmenting LLMs’ training with synthetic regression and math\\ndatasets, during either pre-training or fine-tuning, to future work. We release our code and\\nresults at https://github.com/robertvacareanu/llm4regression .\\nAcknowledgments\\nThis work was partially supported by the Defense Advanced Research Projects Agency\\n(DARPA) under the ASKEM and Habitus programs. Mihai Surdeanu declares a financial\\ninterest in lum.ai . This interest has been properly disclosed to the University of Arizona\\nInstitutional Review Committee and is managed in accordance with its conflict of interest\\npolicies.\\n8 Ethics Statement\\nIn this work we explored the extent to which large language models (LLMs) are able to\\nperform regression tasks. We did not perform any additional training. We do not envision\\nany negative impact of our results.\\n9 Limitation\\nThis study focuses primarily on regression tasks, including an exploration into regression-\\nlike scenarios where inputs are symbolic rather than numeric, yet the outputs remain\\nnumeric.\\nA second limitation is the reliance on several large language models, including proprietary\\nones whose performance may change over time, potentially affecting reproducibility. To\\n11'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 11}, page_content='Preprint. A version of this work is under review.\\naddress this, we also included leading open-weight models in our analysis, though we note\\nthat their performance is generally behind that of private models. Additionally, we release\\nour intermediate results.\\nThird, the issue of data contamination poses a challenge, given the opaque nature of training\\ndatasets for many LLMs. We have taken several steps to mitigate this risk: (i) Our analysis\\nspans multiple LLMs, reducing the likelihood of all models being contaminated in the same\\nway; (ii) We evaluated models with multiple random seeds on newly introduced datasets\\n(alongside known ones like Friedman #1 ). In this way, we diminish the chance that these\\nmodels have been directly exposed to the exact datasets during training; (iii) We included\\nresults with Falcon 40B, whose training data is publicly available (please see Appendix N\\nfor more details). We acknowledge that these measures do not eliminate the potential for\\ndata contamination entirely.\\nFourth, while we showed empirical evidence that large language models are capable to\\nperform regression tasks, we did not provide theoretical explanations to support these\\nobservations.\\nReferences\\nOpenAI Josh Achiam, Steven Adler, Sandhini Agarwal, and Lama Ahmad et al. Gpt-4\\ntechnical report. 2023. URL https://api.semanticscholar.org/CorpusID:257532815 .\\nKwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to\\nimplement preconditioned gradient descent for in-context learning. In Thirty-seventh\\nConference on Neural Information Processing Systems , 2023. URL https://openreview.net/\\nforum?id=LziniAXEI9 .\\n01. AI, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, and Guanwei Zhang\\net al. Yi: Open foundation models by 01.ai, 2024.\\nEkin Aky ¨urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learn-\\ning algorithm is in-context learning? investigations with linear models. In The Eleventh\\nInternational Conference on Learning Representations , 2023. URL https://openreview.net/\\nforum?id=0g0X4H8yN4I .\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,\\nRuxandra-Aim ´ee Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele\\nMazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon\\nseries of open language models. ArXiv , abs/2311.16867, 2023. URL https://api.\\nsemanticscholar.org/CorpusID:265466629 .\\nAnthropic. The claude 3 model family: Opus, sonnet, haiku. URL https://api.\\nsemanticscholar.org/CorpusID:268232499 .\\nYu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians:\\nProvable in-context learning with in-context algorithm selection. In Workshop on Efficient\\nSystems for Foundation Models @ ICML2023 , 2023. URL https://openreview.net/forum?\\nid=vlCG5HKEkI .\\nYoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: A practical\\nand powerful approach to multiple testing. Journal of the Royal Statistical Society. Series\\nB (Methodological) , 57(1):289–300, 1995. ISSN 00359246. URL http://www.jstor.org/\\nstable/2346101 .\\nSatwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade. Understanding\\nin-context learning in transformers and LLMs by learning to learn discrete functions.\\nInThe Twelfth International Conference on Learning Representations , 2024. URL https:\\n//openreview.net/forum?id=ekeyCgeRfC .\\nL. Breiman. Random forests. Machine Learning , 45:5–32, 2001. URL https://api.\\nsemanticscholar.org/CorpusID:89141 .\\n12'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 12}, page_content='Preprint. A version of this work is under review.\\nLeo Breiman. Bagging predictors. Machine Learning , 24(2):123–140, 1996. ISSN 1573-0565.\\ndoi: 10.1007/bf00058655. URL http://dx.doi.org/10.1007/BF00058655 .\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\\nRamesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are\\nfew-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin\\n(eds.), Advances in Neural Information Processing Systems , volume 33, pp. 1877–1901. Curran\\nAssociates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/\\n2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf .\\nStephanie C. Y. Chan, Adam Santoro, Andrew K. Lampinen, Jane X. Wang, Aaditya Singh,\\nPierre H. Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive\\nemergent in-context learning in transformers, 2022.\\nXiang Cheng, Yuxin Chen, and Suvrit Sra. Transformers implement functional gradient\\ndescent to learn non-linear functions in context. ArXiv , abs/2312.06528, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:266162320 .\\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why\\ncan GPT learn in-context? language models secretly perform gradient descent as meta-\\noptimizers. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Findings of\\nthe Association for Computational Linguistics: ACL 2023 , pp. 4005–4019, Toronto, Canada,\\nJuly 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.\\n247. URL https://aclanthology.org/2023.findings-acl.247 .\\nOlive Jean Dunn. Multiple comparisons among means. Journal of the American Statistical Asso-\\nciation , 56:52–64, 1961. URL https://api.semanticscholar.org/CorpusID:122009246 .\\nChelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning.\\nIn Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th Inter-\\nnational Conference on Machine Learning , volume 97 of Proceedings of Machine Learning\\nResearch , pp. 1920–1930. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/\\nv97/finn19a.html .\\nJerome H. Friedman. Multivariate Adaptive Regression Splines. The Annals of Statistics ,\\n19(1):1 – 67, 1991. doi: 10.1214/aos/1176347963. URL https://doi.org/10.1214/aos/\\n1176347963 .\\nJerome H. Friedman. Greedy function approximation: A gradient boosting machine. Annals\\nof Statistics , 29:1189–1232, 2001. URL https://api.semanticscholar.org/CorpusID:\\n39450643 .\\nShivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers\\nlearn in-context? a case study of simple function classes. ArXiv , abs/2208.01066, 2022.\\nURL https://api.semanticscholar.org/CorpusID:251253368 .\\nShahriar Golchin and Mihai Surdeanu. Time travel in LLMs: Tracing data contamination in\\nlarge language models. In The Twelfth International Conference on Learning Representations ,\\n2024. URL https://openreview.net/forum?id=2Rwq6c3tvr .\\nTianyu Guo, Wei Hu, Song Mei, Huan Wang, Caiming Xiong, Silvio Savarese, and Yu Bai.\\nHow do transformers learn in-context beyond simple functions? a case study on learning\\nwith representations. In The Twelfth International Conference on Learning Representations ,\\n2024. URL https://openreview.net/forum?id=ikwEDva1JZ .\\nXiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and\\nTianlu Wang. Understanding in-context learning via supportive pretraining data. In\\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st\\n13'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 13}, page_content='Preprint. A version of this work is under review.\\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp.\\n12660–12673, Toronto, Canada, July 2023. Association for Computational Linguistics. doi:\\n10.18653/v1/2023.acl-long.708. URL https://aclanthology.org/2023.acl-long.708 .\\nRoee Hendel, Mor Geva, and Amir Globerson. In-context learning creates task vectors.\\nIn Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for\\nComputational Linguistics: EMNLP 2023 , pp. 9318–9333, Singapore, December 2023. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.624. URL\\nhttps://aclanthology.org/2023.findings-emnlp.624 .\\nK. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal\\napproximators. Neural Netw. , 2(5):359–366, jul 1989. ISSN 0893-6080.\\nYu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. ArXiv ,\\nabs/2310.05249, 2023. URL https://api.semanticscholar.org/CorpusID:263829335 .\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,\\nand Chris Bamford et al. Mixtral of experts. ArXiv , abs/2401.04088, 2024. URL https:\\n//api.semanticscholar.org/CorpusID:266844877 .\\nAlbert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\\ndra Singh Chaplot, and Diego de Las Casas et al. Mistral 7b. ArXiv , abs/2310.06825, 2023.\\nURL https://api.semanticscholar.org/CorpusID:263830494 .\\nJannik Kossen, Yarin Gal, and Tom Rainforth. In-context learning learns label relation-\\nships but is not conventional learning. In The Twelfth International Conference on Learning\\nRepresentations , 2024. URL https://openreview.net/forum?id=YPIA7bgd5y .\\nHongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin-Yu Chen. Training nonlinear\\ntransformers for efficient in-context learning: A theoretical learning and generalization\\nanalysis. ArXiv , abs/2402.15607, 2024.\\nSheng Liu, Lei Xing, and James Y. Zou. In-context vectors: Making in context learning more\\neffective and controllable through latent space steering. ArXiv , abs/2311.06668, 2023.\\nURL https://api.semanticscholar.org/CorpusID:265149781 .\\nArvind V . Mahankali, Tatsunori Hashimoto, and Tengyu Ma. One step of gradient de-\\nscent is provably the optimal in-context learner with one layer of linear self-attention.\\nInThe Twelfth International Conference on Learning Representations , 2024. URL https:\\n//openreview.net/forum?id=8p3fu56lKc .\\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi,\\nand Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context\\nlearning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings\\nof the 2022 Conference on Empirical Methods in Natural Language Processing , pp. 11048–\\n11064, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\\nLinguistics. doi: 10.18653/v1/2022.emnlp-main.759. URL https://aclanthology.org/\\n2022.emnlp-main.759 .\\nSuvir Mirchandani, Fei Xia, Pete Florence, brian ichter, Danny Driess, Montserrat Gonzalez\\nArenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general\\npattern machines. In 7th Annual Conference on Robot Learning , 2023. URL https://\\nopenreview.net/forum?id=RcZMI8MSyE .\\nTomer Bar Natan, Gilad Deutch, Nadav Magar, and Guy Dar. In-context learning and\\ngradient descent revisited. ArXiv , abs/2311.07772, 2023.\\nFrancesco Orabona. A modern introduction to online learning. ArXiv , abs/1912.13213, 2019.\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, and Pamela et al.\\nMishkin. Training language models to follow instructions with human feedback. In\\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in\\nNeural Information Processing Systems , volume 35, pp. 27730–27744. Curran Associates,\\nInc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/\\nb1efde53be364a73914f58805a001731-Paper-Conference.pdf .\\n14'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 14}, page_content='Preprint. A version of this work is under review.\\nJane Pan, Tianyu Gao, Howard Chen, and Danqi Chen. What in-context learning “learns” in-\\ncontext: Disentangling task recognition and task learning. In Anna Rogers, Jordan Boyd-\\nGraber, and Naoaki Okazaki (eds.), Findings of the Association for Computational Linguistics:\\nACL 2023 , pp. 8298–8319, Toronto, Canada, July 2023. Association for Computational\\nLinguistics. doi: 10.18653/v1/2023.findings-acl.527. URL https://aclanthology.org/\\n2023.findings-acl.527 .\\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel,\\nP . Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\\nM. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine\\nLearning Research , 12:2825–2830, 2011.\\nBo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman,\\nHuanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella,\\nKranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming\\nKong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand\\nMom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Wo ´zniak,\\nZhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs\\nfor the transformer era. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings\\nof the Association for Computational Linguistics: EMNLP 2023 , pp. 14048–14077, Singapore,\\nDecember 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\\nfindings-emnlp.936. URL https://aclanthology.org/2023.findings-emnlp.936 .\\nMichael Poli, Stefano Massaroli, Eric Q. Nguyen, Daniel Y. Fu, Tri Dao, Stephen A. Baccus,\\nYoshua Bengio, Stefano Ermon, and Christopher R ´e. Hyena hierarchy: Towards larger\\nconvolutional language models. In International Conference on Machine Learning , 2023a.\\nURL https://api.semanticscholar.org/CorpusID:257050308 .\\nMichael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and\\nArmin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Process-\\ning Models, 12 2023b. URL https://github.com/togethercomputer/stripedhyena .\\nYasaman Razeghi, Robert L Logan IV , Matt Gardner, and Sameer Singh. Impact of pre-\\ntraining term frequencies on few-shot numerical reasoning. In Yoav Goldberg, Zornitsa\\nKozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics:\\nEMNLP 2022 , pp. 840–854, Abu Dhabi, United Arab Emirates, December 2022. Associ-\\nation for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.59. URL\\nhttps://aclanthology.org/2022.findings-emnlp.59 .\\nBaptiste Rozi `ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, and Xiaoqing Tan\\net al. Code llama: Open foundation models for code. ArXiv , abs/2308.12950, 2023. URL\\nhttps://api.semanticscholar.org/CorpusID:261100919 .\\nDavid Ruppert. The elements of statistical learning: Data mining, inference, and pre-\\ndiction. Journal of the American Statistical Association , 99:567 – 567, 2004. URL https:\\n//api.semanticscholar.org/CorpusID:118901444 .\\nOscar Sainz, Jon Campos, Iker Garc ´ıa-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and\\nEneko Agirre. NLP evaluation in trouble: On the need to measure LLM data contamina-\\ntion for each benchmark. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings\\nof the Association for Computational Linguistics: EMNLP 2023 , pp. 10776–10787, Singa-\\npore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.\\nfindings-emnlp.722. URL https://aclanthology.org/2023.findings-emnlp.722 .\\nRobert E. Schapire. The strength of weak learnability. Machine Learning , 5:197–227, 1989.\\nURL https://api.semanticscholar.org/CorpusID:6207294 .\\nLingfeng Shen, Aayush Mishra, and Daniel Khashabi. Do pretrained transformers really\\nlearn in-context by gradient descent? ArXiv , abs/2310.08540, 2023. URL https://api.\\nsemanticscholar.org/CorpusID:268499126 .\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, and Jean-Baptiste Alayrac\\net al. Gemini: A family of highly capable multimodal models, 2023.\\n15'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 15}, page_content='Preprint. A version of this work is under review.\\nYuandong Tian, Yiping Wang, Beidi Chen, and Simon Shaolei Du. Scan and snap:\\nUnderstanding training dynamics and token composition in 1-layer transformer. In\\nThirty-seventh Conference on Neural Information Processing Systems , 2023. URL https:\\n//openreview.net/forum?id=l3HUgVHqGQ .\\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, and Yas-\\nmine Babaei et al. Llama 2: Open foundation and fine-tuned chat models. ArXiv ,\\nabs/2307.09288, 2023. URL https://api.semanticscholar.org/CorpusID:259950998 .\\nMax Vladymyrov, Johannes von Oswald, Mark Sandler, and Rong Ge. Linear transformers\\nare versatile in-context learners. ArXiv , abs/2402.14180, 2024.\\nJohannes von Oswald, Eyvind Niklasson, E. Randazzo, Jo ˜ao Sacramento, Alexander Mord-\\nvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context\\nby gradient descent. In International Conference on Machine Learning , 2022. URL\\nhttps://api.semanticscholar.org/CorpusID:254685643 .\\nJohannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas\\nZucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Blaise Ag ¨uera y Arcas, Max Vladymy-\\nrov, Razvan Pascanu, and Jo ˜ao Sacramento. Uncovering mesa-optimization algorithms in\\ntransformers. ArXiv , abs/2309.05858, 2023.\\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanx-\\niao Liu, Da Huang, Denny Zhou, and Tengyu Ma. Larger language models do in-context\\nlearning differently. ArXiv , abs/2303.03846, 2023. URL https://api.semanticscholar.\\norg/CorpusID:257378479 .\\nNoam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning.\\nInThirty-seventh Conference on Neural Information Processing Systems , 2023. URL https:\\n//openreview.net/forum?id=f3JNQd7CHM .\\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of\\nin-context learning as implicit bayesian inference. In International Conference on Learning\\nRepresentations , 2022. URL https://openreview.net/forum?id=RdJVFCHjUMI .\\nYue Xing, Xiaofeng Lin, Namjoon Suh, Qifan Song, and Guang Cheng. Benefits of trans-\\nformer: In-context learning in linear regression tasks with unstructured data. ArXiv ,\\nabs/2402.00743, 2024.\\nRuiqi Zhang, Spencer Frei, and Peter Bartlett. Trained transformers learn linear models\\nin-context. In R0-FoMo:Robustness of Few-shot and Zero-shot Learning in Large Foundation\\nModels , 2023. URL https://openreview.net/forum?id=MpDSo3Rglq .\\nA Appendix Structure\\nWe organized this appendix as follows.\\nIn Appendix B we provide additional details of the datasets we used.\\nIn Appendix C we provide additional details of the models we used.\\nIn Appendix D, we provide additional experiments and results to complement Section 3:\\nLarge Language Models Can Do Linear Regression.\\nIn Appendix E, we provide additional experiments and results to complement Section 4:\\nLarge Language Models Can Do Non-Linear Regression.\\nIn Appendix F, we show the average ranks obtain by each model across different dataset\\ntypes.\\nIn Appendix G, we provide additional experiments and results to complement Section 5:\\nHow Fast Do Large Language Models Adapt?\\nIn Appendix J we detail how LLMs provided justifications for their prediction.\\n16'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 16}, page_content='Preprint. A version of this work is under review.\\nIn Appendix K we include another experiment: regression task with non-numerical inputs.\\nIn Appendix L we analyze the effects of rounding.\\nIn Appendix M we analyze whether the performance of LLMs is similar with KNNs or not.\\nIn Appendix N we analyze whether the results we have seen could be the effect of data\\ncontamination.\\nIn Appendix O we analyze whether the performance of LLMs plateaus after a given number\\nof in-context examples or not.\\nIn Appendix P we analyze the performance of LLMs whose backbone architecture is different\\nfrom Transformers.\\nB Datasets\\nWe provide the formulas for all datasets used below. We set the noise to 0 for all datasets.\\nB.1 Linear Regression\\nIn order to generate the linear regression datasets, we use the function make regression ,\\navailable in sklearn (Pedregosa et al., 2011).\\nB.2 Friedman # 1\\nf(x) =10·sin(x0x1π) +20(x2−0.5)2+10x3+5x4+ϵ· N(0, 1)\\nWhere x0,x1,x2,x3,x4∼U(0, 1)\\nB.3 Friedman # 2\\nf(x) =s\\nx2\\n1+\\x12\\nx2·x3−1\\nx2·x4\\x132\\n+ϵ· N(0, 1)\\nWhere\\nx1∼ U(0, 100 )\\nx2∼ U(40π, 560π)\\nx3∼ U(0, 1)\\nx4∼ U(1, 11)\\nB.4 Friedman # 3\\nf(x) =arctan \\nx1x2−1\\nx1x3\\nx0!\\n+ϵ· N(0, 1).\\nWhere\\nx1∼ U(0, 100 )\\nx2∼ U(40π, 560π)\\nx3∼ U(0, 1)\\nx4∼ U(1, 11)\\n17'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 17}, page_content='Preprint. A version of this work is under review.\\nB.5 Original # 1\\ny=10x+sin(5πx) +cos(6πx) (4)\\nWhere\\nx∼ U(0, 100 ) (5)\\nB.6 Original # 2\\ny= (x4\\n1+ (x2·x3−2√x2·√x4)2)3\\n4\\nWhere\\nx0∼ U(0, 3)\\nx1∼ U(4π, 56π)\\nx2∼ U(0, 2)\\nx3∼ U(1, 11)\\nB.7 Original # 3\\ny=ex0+x1·x2√x3+ (x0·x3)3\\n2\\nWhere\\nx0∼ U(1, 3)\\nx1∼ U(1, 10)\\nx2∼ U(0, 10)\\nx3∼ U(1, 20)\\nB.8 Original # 4\\ny=x1\\n10·sin(x0) +x0\\n10·cos(x1) +√x0log(x1)√x1log(x0)\\nWhere\\nx0,x1∼ U(2, 100 )\\nB.9 Original # 5\\ny=100∗max(so f tmax (x\\n10))\\nWhere\\nx0,x1,x2∼ U(−25, 25 )\\n18'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 18}, page_content='Preprint. A version of this work is under review.\\nB.10 Neural Network Induced\\nFor the random datasets induced by neural networks, we randomly initialize a neural\\nnetwork and create a dataset by feeding random data to it. The dataset Simple Random NN\\n1was created by using a neural network with one hidden layer with ReLU non-linearities.\\nThe dataset Transformer 1 was created by a randomly initialized Transformer encoder\\nblock.\\nB.11 Non-Numerical Regression\\nWe provide the code to generate the non-numerical regression datasets in the Listing 1.\\nEssentially, we assign a random number (0 to 26) to each lowercase letter. Then we sample\\na weight vector. The expected output is generated by doing a dot product between the\\nunderlying assigned value of each character and the generated weight vector.\\nListing 1: The python code to generate non-numerical regression datasets\\n1 import random\\n2 import numpy as np\\n3 import s t r i n g\\n4\\n5 max num vars = 5\\n6 n samples = 51\\n7\\n8 def g e t c h a r a c t e r r e g r e s s i o n ( random state = 1 ) :\\n9 r = random . Random( random state )\\n10 alphabet = l i s t ( s t r i n g . a s c i i l o w e r c a s e )\\n11 s h u f f l e d = r . sample ( alphabet , 26)\\n12 l 2 i = [ ( c , i ) f o r c , i in enumerate ( s h u f f l e d ) ]\\n13 gen = np . random . RandomState ( random state )\\n14\\n15 l e t t e r s = r . sample ( l 2 i , max num vars )\\n16 sample = r . choices ( l e t t e r s , k=max num vars *nsamples )\\n17 sample = np . array ( sample ) [ : , 1 ] . reshape ( −1 , max num vars )\\n18\\n19 weight vec = 10 *gen . uniform ( s i z e =( max num vars ) )\\n20\\n21 i 2 l = d i c t ( l e t t e r s )\\n22 l 2 i = {v : k f o r k , v in i 2 l . items ( ) }\\n23\\n24 y = [ weight vec@np . array ( [ l 2 i [ c ] f o r c in x ] ) f o r x in sample ]\\n19'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 19}, page_content='Preprint. A version of this work is under review.\\nFigure 8: Performance Comparison between LLMs and unsupervised baselines on a non-\\nnumeric regression dataset. LLMs are outperforming our unsupervised heuristics even in\\nthis regime.\\nC Models\\nIn the following, we provide additional details of the models we used for our main ex-\\nperiments and how we used them. We used three different types of models, as follows:\\n(a) Large Language Models, (b) Traditional Supervised Methods, and (c) Heuristic-Based\\nUnsupervised Methods. We describe them bellow.\\nC.1 LLM\\nThis section outlines the 12 Large Language Models (LLMs) featured in our main experi-\\nments, which include a mix of open-weights and private models. We also include additional\\nmodels, such as the newest GPT-4 version ( gpt-4-20240409 ), multiple Claude variants, and\\nthe most powerful model released by Cohere, Cohere Command R Plus. We tried with\\nCohere Command R and Cohere Command and observed their performance to be lower,\\nalbeit still the unsupervised baselines (except for Cohere Command).\\nIn Table 2, we categorize the models by their names, availability of weights, and developers,\\ndividing them into two distinct sections. The first section lists the models featured in the\\nmain paper’s experiments (referenced in Sections 3, 4, and 5). The second section introduces\\nadditional models that were utilized for the extended analysis included in the Appendix.\\nWe list in Table 3 the models we used through OpenAI, together with their corresponding\\nmodel code.8\\nWe list in Table 4 the models we used through OpenRouter, together with their correspond-\\ning model code.9\\nWe list in Table 5 the models we used through DeepInfra, together with their corresponding\\nmodel code.10\\nWe list in Table 6 the models we used through Fireworks, together with their corresponding\\nmodel code.11\\n8https://openai.com/\\n9https://openrouter.ai\\n10https://deepinfra.com\\n11https://fireworks.ai/\\n20'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 20}, page_content='Preprint. A version of this work is under review.\\nModel Name Weights Availabil-\\nityDeveloper\\nGPT-4 (Achiam et al., 2023) Not available OpenAI\\nChat GPT (Ouyang et al., 2022) Not available OpenAI\\nClaude 3 Opus (Anthropic) Not available Anthropic\\nClaude 3 Sonnet (Anthropic) Not available Anthropic\\nGemini Pro (Team et al., 2023) Not available Google\\nMistral Medium Not Available Mistral\\nMixtral Mixture of Experts 8x7B (Jiang et al., 2024) Available Mistral\\nMistral 7B (Jiang et al., 2023) Available Mistral\\nLlama2 70B (Touvron et al., 2023) Available Meta\\nCode Llama2 70B (Rozi `ere et al., 2023) Available Meta\\nYi 34B (AI et al., 2024) Available 01.ai\\nDBRX#Available Databricks\\nGPT-4 (20240409) (Achiam et al., 2023) Not available OpenAI\\nGPT-3 Davinci (Brown et al., 2020) Not available OpenAI\\nGPT-3 Babbage (Brown et al., 2020) Not available OpenAI\\nClaude 3 Haiku (Anthropic) Not available Anthropic\\nClaude v2.1†Not available Anthropic\\nClaude v2.0†Not available Anthropic\\nClaude v1.2‡Not available Anthropic\\nCohere Command R Plus◦Available Cohere\\nMixtral Mixture of Experts 8x22B Available Mistral\\nFalcon 40B Almazrouei et al. (2023) Available TII\\nFalcon 40B Instruct Almazrouei et al. (2023) Available TII\\nRWKV v4 14B Peng et al. (2023) Available Various*\\nStripedHyena Nous 7B Poli et al. (2023b) Available TogetherAI\\n†https://www.anthropic.com/news/claude-2\\n‡https://www.anthropic.com/news/releasing-claude-instant-1-2\\n#https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm\\n◦https://cohere.com/command\\n*Developed collaboratively by numerous contributors across 29 different affiliations\\nTable 2: Details of the large language models (LLMs) used in our study, divided into two\\nsections. The first section contains the LLMs used for the experiments from the main body\\nof the paper, while the second section includes additional models explored in the extended\\nresults presented in the Appendix.\\nModel Name API Model Code\\nGPT-4 gpt-4-0125-preview\\nChat GPT gpt-3.5-turbo-1106\\nGPT-4 (20240409) gpt-4-turbo-2024-04-09\\nGPT-3 Davinci davinci-002\\nGPT-3 Babbage babbage-002\\nTable 3: The specific model used for each model family through the OpenAI API.\\nC.1.1 Prompt\\nWe show the prompt we used in Figure 9. Importantly, we used the same prompt for all\\nlarge language models. We did not tune the prompt.\\nWe encountered cases where the large language model would not produce a valid output.\\nFor instance, some models would occasionally output an empty string (i.e., “”). In the\\n21'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 21}, page_content='Preprint. A version of this work is under review.\\nModel Name API Model Code\\nClaude 3 Opus anthropic/claude-3-opus\\nClaude 3 Sonnet anthropic/claude-3-sonnet\\nGemini Pro google/gemini-pro\\nMistral Medium mistralai/mistral-medium\\nClaude 3 Haiku anthropic/claude-3-haiku\\nClaude v2.1 anthropic/claude-2.1\\nClaude v2.0 anthropic/claude-2.0\\nClaude v1.2 anthropic/claude-1.2\\nCohere Command R Plus cohere/command-r-plus\\nStripedHyena Nous 7B togethercomputer/stripedhyena-nous-7b\\nTable 4: The specific model used for each model family through the OpenRouter API.\\nModel Name API Model Code\\nMixtral Mixture of Ex-\\nperts 8x7Bmistralai/Mixtral-8x7B-Instruct-v0.1\\nMistral 7B mistralai/Mistral-7B-Instruct-v0.1\\nLlama 70B meta-llama/Llama-2-70b-chat-hf\\nCode Llama 70B codellama/CodeLlama-70b-Instruct-hf\\nYi 34B 01-ai/Yi-34B-Chat\\nMistral 7B v2 mistralai/Mistral-7B-Instruct-v0.2\\nTable 5: The specific model used for each model family through the DeepInfra API.\\nfollowing, we detail the way we handled them across the experiments we showed in this\\npaper.\\nFor the experiments performed with 100 random seeds and 50 random input-output tuples\\n(i.e.,D50), we simply skip the invalid generations. This is not problematic because it rarely\\noccurs. For example, it has never occurred for the any of the experiments showed in\\nSection 3 and Section 4. Moreover, given that we run each experiment with 100 random\\nseeds, skipping the invalid generations will still leave us with enough samples to estimate\\nthe statistics. For example, out of the results presented in Section 3 and 4, the largest number\\nof invalid generations for a given dataset was 11, by Mistral Medium on Regression NI\\n1/2. Claude 3 Opus produced only 2 invalid generations, for Friedman #2 , and GPT-4 did\\nnot produce any invalid generations. One exception is for the results shown in Appendix H,\\nas we observed that Claude 2.0 and Claude 2.1 produced invalid generations very often.\\nFor example, Claude 2.1 generated invalid generations for the dataset Regression 1/1 in\\n71% of the cases. The reason for invalid generations was usually because Claude 2.1 refused\\nto give “potentially misleading numerical estimates without proper context”. A second\\nexception is for Striped Hyena Nous 7B (Appendix P). We add a cross ( ×) to the cell in the\\nrank heatmap for every model-dataset pair where the number of valid generations is under\\n20.\\nFor the experiments where we investigated how the performance of the models scale with\\nthe number of examples, we average 3 random runs for each dataset size. In this set of\\nexperiments, only Llama 70B generated invalid outputs a total of 3 times, for Original #2\\nandFriedman #2 . We skip the random runs with invalid generations.\\nC.2 Traditional Supervised Models\\nWe use a total of 11 traditional supervised methods, resulting in over 20 different configura-\\ntions. Specifically, we used the following models:\\n22'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 22}, page_content='Preprint. A version of this work is under review.\\nModel Name API Model Code\\nDBRX accounts/fireworks/models/dbrx-instruct\\nMixtral 8x22B accounts/fireworks/models/mixtral-8x22b\\nTable 6: The specific model used for each model family through the Fireworks API.\\nThe task is to provide your best estimate for ”Output”. Please\\nprovide that and only that, without any additional text.\\nFeature 0: -2.06\\nOutput: -81.93\\nFeature 0: -0.64\\nOutput: -25.33\\nFeature 0: 1.62\\nOutput:\\nFigure 9: The prompt we use for all LLMs. Concretely, we use an initial instruction to\\nprevent the models from explaining their prediciton, something which we observed to\\nhappen for some models (e.g., Claude 3 Opus). Then, we give each input-output pair and\\nfinally ask the model to predict the value corresponding to the test input.\\n1.Linear Regression: We used 4 variants of Linear Regression: (i) standard linear\\nregression (Linear Regression), (ii) ridge (Ridge), (iii) lasso (Lasso), and (iv) Linear\\nRegression with Polynomial Features (Linear Regression + Poly), where we used\\npolynomial features of degree 2\\n2.Multi-Layer Perceptron: We used 6 variants of multi-layer preceptrons: 3 with\\ndifferent widths (MLP Wide 1, MLP Wide 2, MLP Wide 3) and 3 with different\\ndepths (MLP Deep 1, MLP Deep 2, MLP Deep 3).\\n3. Random Forest\\n4. Bagging\\n5. Gradient Boosting\\n6. AdaBoost\\n7.SVM: We used both a single SVM and an SVM paired with a Scaler (SVM + Scaler)\\n8.KNN: We used multiple variants of KNN, where we vary the number of neighbors,\\nthe type of distance used, and the power parameter for the Minkowski metric; We\\ndistinguish between them with a v{index}.\\n9. Kernel Ridge\\n10. Spline\\nWe used the sklearn implementation for each model.12Similar to the LLM case, we do not\\ntune any hyperparameters. We use the default hyperparameters available in sklearn. We\\nremark that these supervised baselines are very strong, as (1) many of them are the results\\nof algorithms specifically designed for regression (e.g., Splines), (2) all perform parameter\\nupdates, and (3) the default hyperparameters, as set in widely-used statistical packages,\\nhave been refined over time to offer a reliable and generally strong performance across a\\nvariety of scenarios.\\n12We used sklearn 1.4.1.post1 .\\n23'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 23}, page_content='Preprint. A version of this work is under review.\\nC.3 Unsupervised Models\\nWe use three heuristic inspired unsupervised models:\\n1.Average: Predicts the next value, yn+1, as the mean of all preceding outcomes:\\nyn+1=1\\nn∑n\\ni=1yi.\\n2.Last: Uses the most recent observation tuple (xn,yn)for prediction, such that\\nyn+1=yn.\\n3.Random: Predicts yn+1by randomly selecting from the set of prior observations\\n{y1, . . . , yn}. The final prediction is thus yn+1=sample ([y1, . . . , yn])\\nThe goal of these unsupervised models is to better put the performance obtained by LLMs\\ninto perspective.\\nD Large Language Models Can Do Linear Regression (Expanded)\\nWe expand the barplots shown in Figure 2 with more models and more datasets. In particular,\\nwe show in Figures 10, 11, 12, 13, 14, 15 the performance of the models on six datasets for\\nlinear regression. Specifically, we used the following datasets:\\n1. Regression 1/1, a linear regression with only 1 variable, which is informative\\n2.Regression 1/2, a linear regression with 2 variables, and only 1 informative variable\\n3.Regression 1/3, a linear regression with 3 variables, and only 1 informative variable\\n4. Regression 2/2, a linear regression with 2 variables, both which are informative\\n5.Regression 2/3, a linear regression with 3 variables, and only 2 informative variables\\n6. Regression 3/3, a linear regression with 3 variables, all which are informative\\nWe included the corresponding rank heatmap in Figure 16.\\nWe make the following observations. First, Claude 3 Opus performs among the best for\\nthe linear regression case where there is only one informative variable ( Regression 1/1 ,\\nRegression 1/2 ,Regression 1/3 ), ranking among top 3 best performing models. The\\nperformance drops when there are more informative variables.\\nSecond, we remark that all large language models perform better than all the unsupervised\\nmethods on all datasets.\\nThird, we remark that the large language models display a good overall performance. For\\nexample, there are 4 LLMs (i.e., Claude 3 Opus, Claude 3 Sonnet, GPT-4, DBRX) which\\nperform better than all 3 variants of KNN over all the datasets used.\\nFourth, there are specific LLMs which always perform better than Gradient Boosting, such\\nas Claude 3 Opus and GPT-4. We remark that DBRX and Code Llama 70B outperform\\nGradient Boosting in 4 out of 6 datasets.\\nFigure 10: Regression 1/1\\n24'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 24}, page_content='Preprint. A version of this work is under review.\\nFigure 11: Regression 1/2\\nFigure 12: Regression 1/3\\nFigure 13: Regression 2/2\\nFigure 14: Regression 2/3\\nFigure 15: Regression 3/3\\n25'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 25}, page_content='Preprint. A version of this work is under review.\\nFigure 16: The rank of each model investigated on linear regression datasets. (best viewed in\\ncolor)\\n26'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 26}, page_content='Preprint. A version of this work is under review.\\nE Large Language Models Can Do Non-Linear Regression (Expanded)\\nWe expand the barplots shown in Figure 4 with more models and more datasets. In particular,\\nwe show in Figures 17, 18, 19, 20, 21, 22, 23, and 24 the performance of the models on the\\neight datasets for non-linear regression.\\nAdditionally, we include in Figures 25 and 26 the performance of the models on datasets\\ngenerated by randomly initialized neural networks, similar to the methodology of Garg\\net al. (2022). We can see that the performance of the LLMs decreases on the datasets created\\nusing the neural networks, although it (generally) remains above that of the unsupervised\\nbaselines.\\nSimilar to Figure 6, we include the corresponding rankings in Figure 27.\\nWe would like to remark that Claude 3 Opus obtains an average rank of 7.7, the best out of\\nall models we have investigated. The next best is Gradient Boosting, with 8.1. The next best\\nLLM is Claude 3 Sonnet, with 9.3, then GPT-4 with 12.7.\\nFigure 17: Extended results on the Friedman #1 dataset\\nFigure 18: Extended results on the Friedman #2 dataset\\nFigure 19: Extended results on the Friedman #3 dataset\\n27'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 27}, page_content='Preprint. A version of this work is under review.\\nFigure 20: Extended results on the Original #1 dataset\\nFigure 21: Extended results on the Original #2 dataset\\nFigure 22: Extended results on the Original #3 dataset\\nFigure 23: Extended results on the Original #4 dataset\\nFigure 24: Extended results on the Original #5 dataset\\n28'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 28}, page_content='Preprint. A version of this work is under review.\\nFigure 25: Results on a random regression dataset generated using a randomly initialized\\nNeural Network\\nFigure 26: Results on a random regression dataset generated using a randomly initialized\\nTransformer Encoder Block\\n29'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 29}, page_content='Preprint. A version of this work is under review.\\nFigure 27: Rank of each model investigated on the non-linear regression datasets. (best viewed\\nin color)\\n30'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 30}, page_content='Preprint. A version of this work is under review.\\nF Average Model Ranks\\nTo provide a comprehensive overview of model performance across a diverse array of\\ndatasets, this section aggregates the average ranks obtained by each model. In this sec-\\ntion we show the average ranks obtained by each model across: (1) all linear regression\\ndatasets ( Linear ), (2) all original benchmarking datasets introduced by us ( Original ), (3) all\\nbenchmarking datasets introduced by Friedman ( Friedman ), (4) all neural network induced\\ndatasets ( NN), (5) all non-linear datasets ( Non-Linear ), (6) all datasets ( Overall ).\\nWe show our results in Table 7. We divide the table into three blocks, separated by horizontal\\nlines, corresponding to the results for (1) LLMs (e.g., GPT-4), (2) Traditional Supervised\\nMethods (e.g., Gradient Boosting), and (3) Unsupervised Baselines (e.g., Average). We\\nremark that LLMs obtain, overall, a strong performance. For example, Claude 3 Opus ranks\\nsecond overall, outperforming methods such as Gradient Boosting, KNN, or multi-layer\\nperceptrons. Overall, this strong performance is present in both private and open models.\\nFor example, DBRX and Mixtral 8x22B achieve average ranks that surpass conventional\\nmethods including AdaBoost, KNN, or Random Forests.\\nModel Linear Original Friedman NN Non-Linear Overall\\nClaude 3 Opus 7.67 ±5.89 7.2±12.76 4.00±3.61 17.00±2.83 8.2±9.99 8.00±8.45\\nClaude 3 Sonnet 13.33 ±4.89 7.4 ±10.41 5.67±6.35 21.5 ±0.71 9.7 ±9.82 11.06 ±8.31\\nClaude 3 Haiku 13.33 ±7.69 18.00 ±8.63 19.00 ±10.44 29.5 ±0.71 20.6 ±8.92 17.88 ±8.98\\nGPT-4 12.83 ±4.07 11.4 ±8.02 11.33 ±11.85 22.5 ±0.71 13.6 ±9.05 13.31 ±7.4\\nGPT-4 (20240409) 13.5 ±3.73 12.00 ±9.41 11.33 ±10.21 25.00 ±1.41 14.4 ±9.7 14.06 ±7.83\\nChat GPT 29.5 ±3.33 30.6 ±9.42 24.67 ±12.9 39.00 ±2.83 30.5 ±10.23 30.12 ±8.17\\nDavinci 002 26.17 ±6.27 21.8 ±9.2 23.33 ±5.03 36.00 ±1.41 25.1 ±8.77 25.5 ±7.72\\nBabbage 002 40.17 ±1.83 25.8 ±6.06 39.67 ±0.58 35.5 ±4.95 31.9 ±7.92 35.00 ±7.47\\nGemini Pro 16.5 ±7.4 15.2 ±10.92 21.33 ±7.09 26.5 ±0.71 19.3 ±9.3 18.25 ±8.49\\nMistral Medium 23.17 ±6.71 20.6 ±7.7 20.33 ±5.13 27.5 ±0.71 21.9 ±6.4 22.38 ±6.32\\nMixtral 8x22B 20.5 ±5.65 12.4 ±4.51 16.67 ±11.72 23.5 ±0.71 15.9 ±7.71 17.62 ±7.18\\nMixtral 8x7B 27.17 ±4.92 22.4 ±6.66 26.00 ±9.54 29.00 ±1.41 24.8 ±6.91 25.69 ±6.17\\nMistral 7Bv2 35.00 ±2.53 31.6 ±4.72 26.33 ±7.64 43.5 ±2.12 32.4 ±7.96 33.38 ±6.47\\nMistral 7B 37.33 ±2.66 35.2 ±4.97 36.67 ±7.57 41.5 ±0.71 36.9 ±5.49 37.06 ±4.52\\nDBRX 18.67 ±1.51 15.00 ±8.0 15.67 ±13.43 25.00 ±0.0 17.2 ±9.25 17.75 ±7.25\\nCohere Command R Plus 25.00 ±6.16 26.4 ±8.79 24.67 ±10.07 39.00 ±1.41 28.4 ±9.43 27.12 ±8.3\\nLlama2 70B Chat HF 31.83 ±3.92 32.8 ±7.22 28.67 ±3.51 40.5 ±3.54 33.1 ±6.79 32.62 ±5.76\\nCode Llama 70B 24.5 ±2.88 21.2 ±4.87 23.67 ±11.59 30.00 ±1.41 23.7 ±7.27 24.00 ±5.89\\nYi 34B Chat 28.00 ±2.76 21.2 ±9.36 25.00 ±3.46 33.00 ±2.83 24.7 ±8.04 25.94 ±6.64\\nGradient Boosting 24.83 ±7.41 9.6 ±7.33 8.67 ±5.51 8.5 ±0.71 9.1 ±5.57 15.00±9.94\\nAdaBoost 29.00 ±8.63 17.6 ±4.62 16.00 ±4.58 14.00 ±0.0 16.4 ±4.03 21.12 ±8.62\\nBagging 27.00 ±6.48 14.6 ±4.93 12.00 ±8.19 10.5 ±0.71 13.00 ±5.37 18.25 ±8.96\\nKNN 32.67 ±5.05 23.6 ±10.21 27.33 ±8.96 11.00 ±1.41 22.2 ±10.11 26.12 ±9.86\\nKNN v2 29.67 ±4.84 18.2 ±12.87 25.67 ±8.39 10.00 ±1.41 18.8 ±11.07 22.88 ±10.53\\nKNN v3 28.67 ±5.28 18.2 ±14.62 27.33 ±10.79 15.5 ±0.71 20.4 ±12.04 23.5 ±10.65\\nLinear Regression 1.17±0.41 25.8±13.85 16.00 ±5.57 10.00 ±4.24 19.7 ±11.84 12.75 ±13.04\\nLasso 13.5 ±7.66 24.8 ±14.86 30.67 ±15.7 33.5 ±0.71 28.3 ±12.94 22.75 ±13.22\\nRidge 15.33 ±7.28 25.00 ±14.47 16.33 ±4.04 8.5 ±4.95 19.1 ±12.1 17.69 ±10.44\\nLinear Regression + Poly 2.5 ±0.84 16.00±15.87 3.00±2.65 8.00±7.07 10.5 ±12.49 7.5±10.48\\nMLP Deep 1 5.67 ±3.39 16.00 ±12.88 31.33 ±20.11 11.5 ±9.19 19.7 ±15.51 14.44 ±14.05\\nMLP Deep 2 8.83 ±4.17 18.8 ±13.37 28.67 ±20.98 6.00 ±0.0 19.2 ±15.68 15.31 ±13.42\\nMLP Deep 3 11.00 ±3.35 16.2 ±14.77 29.00 ±19.16 7.5 ±0.71 18.3 ±15.66 15.56 ±12.81\\nMLP Wide 1 4.67 ±1.75 18.00 ±14.2 21.67 ±14.29 9.00 ±9.9 17.3 ±12.95 12.56 ±11.9\\nMLP Wide 2 7.83 ±1.83 25.2 ±11.3 25.00 ±19.16 3.00 ±1.41 20.7±15.02 15.88 ±13.34\\nMLP Wide 3 6.33 ±1.21 24.4 ±11.72 27.67 ±23.86 1.00±0.0 20.7±17.25 15.31 ±15.19\\nRandom Forest 31.5 ±7.04 17.2 ±9.58 14.33 ±6.81 17.00 ±0.0 16.3 ±7.27 22.00 ±10.3\\nKernel Ridge 14.17 ±6.91 30.00 ±15.72 26.33 ±18.23 34.00 ±19.8 29.7 ±15.33 23.88 ±14.74\\nSVM 44.83 ±2.04 32.8 ±14.57 29.67 ±12.01 11.00 ±11.31 27.5 ±14.77 34.00 ±14.4\\nSVM + Scaler 45.5 ±1.76 33.4 ±15.39 24.33 ±16.65 12.00 ±11.31 26.4 ±15.99 33.56 ±15.68\\nSpline 14.00 ±3.95 29.00 ±6.75 10.33 ±7.77 19.5 ±2.12 21.5 ±10.38 18.69 ±9.16\\nAverage 48.5 ±1.38 34.2 ±17.43 43.00 ±1.73 33.5 ±0.71 36.7 ±12.44 41.12 ±11.32\\nRandom 51.17 ±0.98 44.8 ±3.42 46.67 ±3.21 47.00 ±0.0 45.8 ±2.94 47.81 ±3.56\\nLast 51.33 ±0.52 45.00 ±4.3 47.00 ±2.65 47.00 ±1.41 46.00 ±3.33 48.00 ±3.72\\nTable 7: We show the average rank of each model we investigated, across multiple types of\\ndatasets. We divide the table into three blocks, separated by horizontal lines, corresponding\\nto the results for (1) large language models (e.g., GPT-4), (2) Traditional Supervised Methods\\n(e.g., Gradient Boosting), and (3) Unsupervised Baselines (e.g., Average). Overall, the large\\nlanguage models (LLMs) obtain a strong performance. For example, Claude 3 Opus ranks\\nsecond overall, outperforming very strong methods like Gradient Boosting or multi-layer\\nperceptron. This strong performance is present in both private (e.g., Claude 3 Opus, GPT-4)\\nand open-weights models (e.g., DBRX, Mixtral 8x22B).\\n31'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 31}, page_content='Preprint. A version of this work is under review.\\nG How Fast Do Large Language Models Adapt? (Expanded)\\nWe expand Table 1 to include more models. We show the corresponding results in Table 8.\\nAdditionally, we include curve fit plots for models. To keep the number of plots to a\\nmanageable amount, we selected a subset of the models as follows. We selected Claude 3\\nOpus and GPT-4, as they are the flagship closed-source models. We selected Yi 34B Chat\\nfor the open-weights model. Lastly, we selected Gradient Boosting, Linear Regression, and\\nLinear Regression + Poly. We present the corresponding plots in Figure 28 and 29. Since in\\nthese experiments we vary the number of in-context examples starting from 1, we could not\\ninclude variants of KNN that uses more than one neigbhors for their prediction. We included\\nKNN v4 which uses only one neighbor. Additionally, we included KNN v5 , where we use a\\nsmall number of neighbors when the amount of data is small, then gradually increase it. We\\ncan see that their performance is much worse than that of the LLMs, suggesting that the\\nLLMs are doing something more than what KNN do.\\nModel \\\\Dataset Friedman 1 Friedman 2 Friedman 3 Original 1 Original 2 Regression NI 1/3 Regression NI 2/2\\nClaude 3 Opus linear sqrt sqrt log sqrt log log\\nClaude 3 Sonnet linear log linear log log log sqrt\\nGPT-4 linear sqrt sqrt log sqrt log sqrt\\nChat GPT linear linear linear sqrt linear sqrt sqrt\\nGemini Pro linear sqrt linear log sqrt sqrt sqrt\\nMistral Medium linear linear linear sqrt sqrt sqrt sqrt\\nLlama2 70B Chat HF linear linear linear sqrt linear linear sqrt\\nCode Llama 70B linear sqrt linear sqrt sqrt linear sqrt\\nMistral 7B linear linear linear sqrt linear linear linear\\nMixtral 8x7B linear linear linear sqrt linear linear sqrt\\nYi 34B Chat linear sqrt linear sqrt sqrt sqrt sqrt\\nDBRX linear log linear log sqrt sqrt sqrt\\nAdaBoost linear sqrt linear sqrt sqrt sqrt sqrt\\nBagging linear sqrt linear log sqrt sqrt sqrt\\nGradient Boosting sqrt sqrt linear log sqrt log sqrt\\nLinear Regression linear linear linear linear linear log log\\nLasso linear linear linear linear linear log log\\nLinear Regression + Poly sqrt log log linear log log log\\nMLP Deep 1 linear linear linear linear sqrt log log\\nMLP Deep 2 linear linear linear linear sqrt log log\\nMLP Deep 3 linear linear sqrt linear sqrt log log\\nMLP Wide 1 linear linear linear linear sqrt log log\\nMLP Wide 2 sqrt linear linear linear linear log log\\nMLP Wide 3 sqrt linear linear linear linear log log\\nRandom Forest linear sqrt linear sqrt sqrt sqrt linear\\nRidge linear sqrt linear linear linear log log\\nSVM linear linear linear sqrt linear linear linear\\nKernel Ridge linear linear linear linear linear log log\\nSVM + Scaler linear linear linear sqrt linear linear linear\\nKNN v4 linear linear linear log linear sqrt sqrt\\nKNN v5 linear linear linear log linear sqrt sqrt\\nAverage linear linear linear linear linear linear linear\\nLast linear linear linear linear linear linear linear\\nRandom linear linear linear linear linear linear linear\\nTable 8: We show which curve-fit obtained the highest R2for multiple models and datasets,\\nexpanding on Table 1. The slower the growth of the function, the better (i.e., log>sqrt>\\nlinear ).(best viewed in color)\\n32'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 32}, page_content='Preprint. A version of this work is under review.\\n(a)Claude 3 Opus, Friedman #1\\n (b) GPT-4, Friedman #1\\n (c)Yi 34B, Friedman #1\\n(d) Claude 3 Opus, Friedman #2\\n (e)GPT-4, Friedman #2\\n (f)Yi 34B, Friedman #2\\n(g) Claude 3 Opus, Friedman #3\\n (h) GPT-4, Friedman #3\\n (i)Yi 34B, Friedman #3\\n(j)Claude 3 Opus, Original #1\\n (k) GPT-4, Original #1\\n (l)Yi 34B, Original #1\\n(m) Claude 3 Opus, Original #2\\n (n) GPT-4, Original #2\\n (o) Yi 34B, Original #2\\n(p) Claude 3 Opus, Regression 1/3\\n (q) GPT-4, Regression 1/3\\n (r)Yi 34B, Regression 1/3\\n(s)Claude 3 Opus, Regression 2/2\\n (t)GPT-4, Regression 2/2\\n (u) Yi 34B, Regression 2/2\\nFigure 28: Curve Fits for Claude 3 Opus, GPT-4, and Yi 34B on seven (linear and non-linear)\\ndatasets.\\n33'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 33}, page_content='Preprint. A version of this work is under review.\\n(a)LR, Friedman #1\\n (b) GB, Friedman #1\\n (c)LR + Poly, Friedman #1\\n(d) LR, Friedman #2\\n (e)GB, Friedman #2\\n (f)LR + Poly, Friedman #2\\n(g) LR, Friedman #3\\n (h) GB, Friedman #3\\n (i)LR + Poly, Friedman #3\\n(j)LR, Original #1\\n (k) GB, Original #1\\n (l)LR + Poly, Original #1\\n(m) LR, Original #2\\n (n) GB, Original #2\\n (o) LR + Poly, Original #2\\n(p) LR, Regression 1/3\\n (q) GB, Regression 1/3\\n (r)LR + Poly, Regression 1/3\\n(s)LR, Regression 2/2\\n (t)GB, Regression 2/2\\n (u) LR + Poly, Regression 2/2\\nFigure 29: Curve Fits for Linear Regression (LR), Gradient Boosting (GB), and Linear\\nRegression with Polynomial Features (LR + Poly) on seven (linear and non-linear) datasets.\\n34'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 34}, page_content='Preprint. A version of this work is under review.\\nH Claude Performance\\nFollowing the (perhaps surprisingly) strong performance of the Claude family of large\\nlanguage models on various regression tasks (e.g., Figure 27, when averaging the ranks\\nof each model over each dataset, Claude 3 Opus performs the best), we provide results\\nwith additional models from the Claude family, namely: Claude 1.2, Claude 2.0, Claude\\n2.1, and Claude 3 Haiku. We include a rank heatmap for all the models from the Claude\\nfamily currently available in Figure 30. For comparison, we also included the corresponding\\nperformance of two strong models with open-weights: DBRX and Mixtral 8x7B. Claude\\n2.0 and Claude 2.1 were sometimes generating invalid outputs (e.g., “I apologize, upon\\nreflection I do not feel comfortable providing output values without context. Could we have\\na constructive discussion about the meaning and implications of this exercise?”). Therefore,\\nwe omit those problematic configurations. For all the other cases, the average performance\\nis the result of at least 20 runs.13We note that the performance of Claude 3 models is much\\nbetter than that of older models.\\nFigure 30: The rank of each model from the Claude family currently available. For com-\\nparison, we also included the ranks of two (strong) models with open-weights: DBRX and\\nMixtral 8x7B. (best viewed in color)\\nI Costs\\nWe estimate the total cost for all our experiments to under $1200. We spent approximately\\n$300 on OpenRouter. We spent approximately $600 on OpenAI. The cost for OpenAI is\\nhigher because we used it in our preliminary experiments. Additionally, the preliminary\\nexperiments used gpt-4 , which is more expensive than gpt-4-0125-preview . We switched\\ntogpt-4-0125-preview after we added the additional text to the prompt, instructing the\\nmodel to only output their best estimate.14\\nJ LLMs Justifying Their Prediction\\nWithout adding the prefix instruction text (see prompt in Appendix C.1.1), which instructed\\nthe models to output only its best estimate, some LLMs (e.g., Claude 3 Opus) started to\\nprovide explanations, in an attempt to justify their prediction.15Analyzing these “explana-\\n13These invalid outputs are specific to Claude 2.0 and Claude 2.1. For example, for Claude 3 Opus,\\nthere exist only 2 instances where it does not generate a valid output, out of a total of over 1000 runs.\\n14We did not need this additional instruction in our initial experiments. We added it when we\\nexpanded the number of LLMs used, as some of them (e.g., Claude 3) would provide justifications\\nbefore giving the final output. All models use the same prompt.\\n15We hypothesize that this is because their system prompt instructs them to provide explanations.\\n35'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 35}, page_content='Preprint. A version of this work is under review.\\ntions” revealed that there is a discrepancy between their explanation and their prediction.\\nFor example, for a non-linear regression problem, Claude 3 Opus suggests to train a Linear\\nRegression model and gives the code to do so. Then it elaborates on how to use it for\\ninference and gives the output. However, manually running the code suggested by the\\nmodel results in a very different output. We provide some examples in Figures 31, 32, , 33,\\nand 34. We describe each one below.\\nIn Figure 31, Claude 3 correctly identifies that the output is generated by multiplying the\\ninput with a constant. Then, it calculates the constant and gives the final output.\\nIn Figure 32, we can see that Claude 3 first calculates the mean of each feature. Manually\\ninspecting the true values, they are: Mean Feature 0: 51.3180, Mean Feature 1: 846.6326,\\nMean Feature 2: 0.4764, Mean Feature 3: 5.2438. We remark that the values given by\\nClaude 3 are surprisingly close. Then, Mean Output: 374.04. Then, Claude 3 calculate\\nthe covariance between each feature and the output. These estimates are much worse.\\nFor example, Cov(Feature 0, Output) is actually 2516.3683, not 8958.8469. Next, Claude 3\\ngives the variance of each feature. These values are close. For example, Var(Feature 0) is\\n729.420751 and Claude 3 generates 729.9052. Then, the model calculates the coefficients.\\nThese calculations are close to their true value, except for b0. Lastly, the model gives\\nthe final formula to calculate the output, which is (according to the model): 227.4744 +\\n25.9730 ∗42.54 +1.2777 ∗851.93 +1648.5958 ∗0.51+129.2408 ∗6.26=436.5981. However,\\nthis calculation is wrong. The output of that equation is 4070.70. However, we would like\\nto remark that what the model generated (wrongly, from a mathematical point of view), is\\nactually much closer to the true value of 434.54. In other words, the explanation offered by\\nthe model was not faithful.\\nIn Figure 33, we can see that Claude 3 suggests that there is a strong linear relationship\\n(please refer to Figure 5 for a plot of the data). Then, Claude 3 fits a linear regression\\ny=mx+band gives the approximate values: m=0.9102 and b=12.1615. However,\\nmanually fitting a linear regression model on the corresponding data yields the following\\nvalues m=0.97 and b=2.94. Then, Claude 3 calculates the final output. The calculation\\nis correct, however it is far off from the true value of: 30.86. We would like to remark that\\ninstructing the model to give its best estimate without any additional information gives a\\nmuch better prediction: 30.91.\\nIn Figure 34, the solution generated by Claude 3 involves calculating the nearest neighbor.\\nThe problem in this approach is that the dataset given in-context contain only 50 examples,\\nwhile the solution generated Claude involves taking the examples 50 and 54, which are\\nnon-existent.\\nAll in all, we remark that the explanations provided by the model are not always faithful. We\\nalso remark that the predictions of the model in two cases: (i) when it outputs an explanation\\nand (ii) when it does not output an explanation can vary.\\n36'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 36}, page_content='Preprint. A version of this work is under review.\\nTo find the output for a given Feature 0 value, we need to determine the relationship\\nbetween Feature 0 and the Output. Looking at the provided data points, it appears\\nthat the Output is approximately equal to Feature 0 multiplied by a constant value.\\nLet’s calculate this constant value using a few data points:\\nFor Feature 0 = -2.06, Output = -81.93\\nConstant = Output / Feature 0 = -81.93 / -2.06 ≈39.77\\nFor Feature 0 = 1.46, Output = 58.14\\nConstant = Output / Feature 0 = 58.14 / 1.46 ≈39.82\\nThe constant values are very close, so we can assume that the relationship\\nis:\\nOutput = Feature 0 ·39.8 (rounded to one decimal place)\\nNow, for Feature 0 = 1.62, we can calculate the Output:\\nOutput = 1.62 ·39.8≈64.48\\nTherefore, when Feature 0 is 1.62, the Output should be approximately 64.48.\\nFigure 31: Claude 3 Opus justifying its output on Regression NI 1/1. In this dataset, the\\nfinal output is constructed by multiplying the input feature with a constant value. Claude\\n3 correctly identified the constant. The answer is very close to the correct one (differences\\nmight appear due to rounding).\\n37'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 37}, page_content='Preprint. A version of this work is under review.\\nTo predict the output value, I’ll use a linear regression model with the given features.\\nThe model is:\\nOutput = b0 + b1*Feature0 + b2*Feature1 + b3*Feature2 + b4*Feature3\\nFirst, I’ll calculate the mean of each feature and the output:\\nMean Feature 0: 51.7054\\nMean Feature 1: 836.5202\\nMean Feature 2: 0.4844\\nMean Feature 3: 5.3154\\nMean Output: 385.2352\\nNext, I’ll calculate the covariance between each feature and the output:\\nCov(Feature 0, Output): 18958.8469\\nCov(Feature 1, Output): 262416.1385\\nCov(Feature 2, Output): 138.6469\\nCov(Feature 3, Output): 1117.1448\\nAnd the variance of each feature:\\nVar(Feature 0): 729.9052\\nVar(Feature 1): 205416.6328\\nVar(Feature 2): 0.0841\\nVar(Feature 3): 8.6469\\nThe coefficients are:\\nb1 = Cov(Feature 0, Output) / Var(Feature 0) = 25.9730\\nb2 = Cov(Feature 1, Output) / Var(Feature 1) = 1.2777\\nb3 = Cov(Feature 2, Output) / Var(Feature 2) = 1648.5958\\nb4 = Cov(Feature 3, Output) / Var(Feature 3) = 129.2408\\nb0 = Mean(Output) - b1*Mean(Feature 0) - b2*Mean(Feature 1) - b3*Mean(Feature 2)\\n- b4*Mean(Feature 3)\\n= -227.4744\\nSo, the linear regression model is:\\nOutput = -227.4744 + 25.9730*Feature0 + 1.2777*Feature1 + 1648.5958*Fea-\\nture2 + 129.2408*Feature3\\nPlugging in the given feature values:\\nOutput = -227.4744 + 25.9730*42.54 + 1.2777*851.93 + 1648.5958*0.51 + 129.2408*6.26\\n= 436.5981\\nTherefore, the predicted output value is 436.60.\\nFigure 32: Claude 3 Opus justifying its output on Friedman #2 . Except for covariance and b0,\\nthe calculations made by Claude are close to their true values (e.g., Cov(Feature 0, Output)\\nis 2516.3683, not 8958.8469). The model gives the final formula to calculate the output:\\n227.4744 +25.9730 ∗42.54 +1.2777 ∗851.93 +1648.5958 ∗0.51+129.2408 ∗6.26=436.5981.\\nHowever, this calculation is wrong. The output of that equation is 4070.70. However, we\\nwould like to remark that what the model generated (wrongly, from a mathematical point\\nof view), is actually much closer to the true value of 434.54. In other words, the explanation\\noffered by the model was not faithful, even though the final prediction was good.\\n38'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 38}, page_content='Preprint. A version of this work is under review.\\nTo find the relationship between the input Feature 0 and the Output, let’s plot the\\ndata points and see if there’s a clear pattern or trend.\\nPlotting the data, it appears there is a strong linear relationship between\\nFeature 0 and Output. As Feature 0 increases, the Output also tends to increase in a\\nfairly straight line.\\nTo quantify this linear relationship, we can calculate the line of best fit us-\\ning linear regression. The equation for a line is typically written as:\\ny = mx + b\\nwhere:\\ny is the Output value\\nm is the slope of the line\\nx is the Feature 0 value\\nb is the y-intercept (the Output value when Feature 0 is 0)\\nUsing the given data points, the linear regression equation comes out to ap-\\nproximately:\\nOutput = 0.9102 * Feature 0 + 12.1615\\nSo for the given Feature 0 value of 31.34, plugging that into the equation:\\nOutput = 0.9102 * 31.34 + 12.1615\\n= 28.53 + 12.1615\\n= 40.69\\nTherefore, based on the strong linear trend in the data, the predicted Output value\\ncorresponding to a Feature 0 value of 31.34 would be around 40.69.\\nFigure 33: Claude 3 Opus justifying its output on Original #1. In this dataset, the final\\noutput is constructed as follows: f(x) =10x+sin(5πx) +cos(6πx). The solution proceeds\\nby fitting a linear regression and giving the approximate values. However, manually fitting\\na linear regression model yields the following values: m=0.97 and b=2.94. Then, the\\nmodel proceeds by calculating the final output. The calculation is correct, but the final value\\nis far off from the true value, which is 30.86. We would like to remark that instructing the\\nmodel to give its best estimate without any additional information or explanation gives a\\nmuch better prediction: 30.91.\\n39'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 39}, page_content='Preprint. A version of this work is under review.\\nTo predict the output for the given input features, I’ll calculate the average output\\nfor the two nearest neighbors in the training data.\\nInput: Feature 0: 0.7, Feature 1: -2.03\\nCalculating Euclidean distances to all training examples:\\n1.p\\n(0.7−(−1.06))2+ (−2.03−(−0.91))2=2.27\\n2.p\\n(0.7−0.37)2+ (−2.03−1.36)2=3.45\\n...\\n49.p\\n(0.7−(−0.38))2+ (−2.03−0.07)2=2.35\\n50.p\\n(0.7−1.17)2+ (−2.03−(−2.34))2=0.62\\n51.p\\n(0.7−(−0.46))2+ (−2.03−0.35)2=2.76\\n52.p\\n(0.7−(−0.75))2+ (−2.03−0.01)2=2.64\\n53.p\\n(0.7−0.38)2+ (−2.03−(−0.22))2=1.88\\n54.p\\n(0.7−(−0.4))2+ (−2.03−(−1.38))2=1.41\\nThe two nearest neighbors are examples 50 and 54, with distances 0.62 and\\n1.41.\\nExample 50 output: 31.76\\nExample 54 output: -49.65\\nPredicted output = (31.76 + (-49.65)) / 2 = -8.945\\nSo for input features 0.7 and -2.03, the predicted output using 2-nearest neighbors is\\n-8.945.\\nFigure 34: Claude 3 Opus justifying its output on Regression NI 2/2 , a linear regression\\ndataset with two variables, where both are informative. However, the dataset given as\\nin-context exemplars to Claude contains only 50 data points, while the solution generated\\nby Claude involves taking the examples 50 and 54. Those examples are non-existent.\\n40'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 40}, page_content='Preprint. A version of this work is under review.\\nK Beyond Numerical Regression\\nOur investigation has centered on conventional regression tasks characterized by inputs\\nand outputs represented as numerical values. However, the performance on these tasks\\nmight be influenced by the quality of numerical token embeddings, which can serve as a\\nconfounding factor Razeghi et al. (2022). To address this and broaden our analysis, we shift\\nour focus to datasets generated following the methodology outlined in Section 2.1.3. This\\nallows us to evaluate the models’ capabilities in contexts where inputs are symbolic rather\\nthan numerical.\\nSpecifically, we define an input vocabulary V, consisting of a small number of (random)\\nsymbols.16Each symbol is randomly assigned a number {1,. . ., 26}. We map the symbols to\\na numerical value by sampling a weight vector w∈Rdand doing a dot product between\\nit and the corresponding values of each symbol. We present our results in Figure 8. The\\nlarge language model display, once again, a performance superior to that of unsupervised\\nmodels.\\nL Effects of Rounding\\nDue to computational budgets and context limits,17we rounded both the input and the\\noutput to two decimals. To validate that our conclusions are not an artifact of the rounding\\nmechanism, we re-ran GPT-4 on Friedman #2 and Friedman #3, rounding to five decimals.\\nWe selected GPT-4 because it obtained overall strong results and offers a good latency. We\\nselected Friedman #2 and Friedman #3 because LLMs obtained generally good performance.\\nWe also ran the traditional supervised methods. We include the updated results in Figure 35.\\nComparing it with Figure 27, we can see that the performance of GPT-4 remains strong.\\nThis time it even outperforms Gradient Boosting on Friedman #3 and ranks first. The\\nperformance on Friedman #3 is only under Linear Regression + Poly, similar to Figure 27.\\nAll in all, the strong performance we observed is unlikely to be just an artifact of rounding.\\nFigure 35: The ranks of the models when rounding to five decimals. The performance of\\nGPT-4 remains strong, ranking above many traditional supervised methods. This is in line\\nwith our previous observations, where we rounded to two decimals. (best viewed in color)\\nM Is It Just Better KNN?\\nWe can see from Appendix D and E that the performance of certain LLMs is almost always\\nbetter than that of all three variants of KNN presented. For example, in the case of linear\\nregression, Claude 3 Opus, Claude 3 Sonet and GPT-4 always perform better than the three\\nvariants of KNN we used. Similar, in the case of non-linear regression, only for Simple NN\\n1andTransformer 1 isanyof the KNN variants outperforming Claude 3 Opus. Moreover,\\nfrom Table 8, which records the best curve fit over the cumulative regret, we can see that\\nboth variants of KNN perform worse than Claude 3 Opus or GPT-4.\\n16We used 5 symbols, in the form of letters (e.g., a)\\n17For example, Llama2 context size is only 4096.\\n41'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 41}, page_content='Preprint. A version of this work is under review.\\nFigure 36: Comparison between the ranks of LLMs and the ranks of best KNN on each\\ndataset. (best viewed in color)\\nTo further investigate the extent to which what LLMs are internally implementing to do\\nregression is a variant of KNN, we compare the performance of the LLMs against a total of\\n70 KNNs in a setting similar to that from Section 3 and 4: we randomly sample a dataset\\nof size 50, which is given to both LLMs and KNNs and we ask them to predict the output\\ncorresponding to a testing data point. We repeat each experiment 100 times, with different\\nrandom seeds. We describe what KNNs we considered in Listing 2.\\nListing 2: The python code to create the KNN configurations\\n1 f o r n in [ 1 , 2 , 3 , 5 , 7 , 9 , 1 1 ] :\\n2 f o r w in [ ’ uniform ’ , ’ distance ’ ] :\\n3 f o r p in [ 0 . 2 5 , 0 . 5 , 1 , 1 . 5 , 2 ] :\\n4 y i e l d KNeighborsRegressor ( n neighbors=n , weights=w, p=p )\\nWe summarize our results in Figure 36. To keep the plot comprehensible, we chose the\\nbest-performing KNN configuration for each dataset. However, the ranking considers the\\nperformance of all models. We draw the following conclusions.\\nFirst, we remark that except on Friedman #1 , for every other dataset , the top 9 best per-\\nforming models are all LLMs. In other words, both closed-source models (e.g., Claude 3\\nOpus, GPT-4) and open-weights models (e.g., DBRX, Mixtral) outperform all KNN models\\non all the datasets except Friedman #1 . This suggests that the mechanism implemented by\\nin-context learning might be something more complex than KNN.\\nLast, we remark that for Friedman #1 , only Claude 3 Opus and Claude 3 Sonnet outperform\\nthe KNN variants. Moreover, Claude 3 Opus and Claude 3 Sonnet outperform allKNN\\nvariants we experimented with on alldatasets.\\nN Could It Be Just Contamination?\\nThe very large datasets that are used to train contemporary large language models (LLMs)\\nraise concerns about potential (accidental or not) contamination Sainz et al. (2023); Golchin\\n& Surdeanu (2024). In our study, we have attempted to mitigate this as follows. First, we\\nused many different random seeds. However, this does not nullify the risk that the LLM has\\nseen similar data (e.g., data from Friedman #1 , but with other random seeds). To mitigate\\nthis, we explored the performance of the models on regression functions of our own creation.\\nThis makes it unlikely that the model has seen data coming from the exact same function.\\n42'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 42}, page_content='Preprint. A version of this work is under review.\\nFigure 37: Falcon performance on the Regression NI 1/1 #2 dataset\\nFor example, across all our newly introduced datasets (e.g., Original #1 ,Original #2 ,\\nOriginal #3 ,Original #4 ,Original #5 ), Claude 3 Opus obtains the highest average rank\\nof 6.4. Second place is Clade 3 Sonnet, with an average rank of 6.8, then Gradient Boosting\\nwith 8.4. Furthermore, our empirical evidence of consistent high performance across a\\ndiverse array of LLMs.\\nTo further analyze the data contamination issue, we perform two additional experiments.\\nWe provide results with Falcon, an LLM whose training data is publicly available. Second,\\nwe perform an experiment similar to the approach proposed in Golchin & Surdeanu (2024),\\nwhere we compare the performance of LLMs with and without knowing the dataset where\\nthe data comes from.\\nN.1 LLMs with known Training Data\\nIn this section we expand our analysis to include Falcon 40B and Falcon 40B Instruct,\\ncomparing their performance with both traditional statistical methods and other LLMs. To\\nkeep the figures comprehensible, we added only the following LLMs: Claude 3 Opus, Chat\\nGPT, Mixtral 8x7B, and Mistral 7B.\\nWe remark that the Falcon LLM team has released their training data,18offering further\\ninsights into how the training environments of contemporary LLMs can result into LLMs\\nbeing capable of regression.\\nDue to the context size limitations of Falcon,19we only evaluated it on the linear regression\\ndatasets and on the Original #1 dataset. The other datasets have a larger number of\\ninput variables (e.g., Friedman #2 has 5 input variables) and we could not fit 50 in-context\\nexamples. We show our results on four datasets, Regression NI 1/1 ,Regression NI 1/2 ,\\nRegression NI 2/2 ,Original #1 in Figures 37, 38, 39 and 40. Additionally, we include\\nthe corresponding rank heatmap in Figure 41.\\nWe make the following observations. First, Falcon 40B outperforms our unsupervised\\nbaselines. Second, Falcon 40B outperforms Gradient Boosting and Random Forests on\\nRegression NI 1/1 .\\nOverall, Falcon 40B displays, as well, the capability of doing regression when given in-\\ncontext examples, albeit to a smaller degree than when compared to more powerful (and\\nnewer) models.\\n18https://huggingface.co/datasets/tiiuae/falcon-refinedweb\\n19The context size of Falcon is 2048.\\n43'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 43}, page_content='Preprint. A version of this work is under review.\\nFigure 38: Falcon performance on the Regression NI 1/2 dataset\\nFigure 39: Falcon performance on the Regression NI 2/2 dataset\\nFigure 40: Falcon performance on the Original #1 dataset\\nFigure 41: The rank of Falcon models, compared with traditional supervised methods and\\nunsupervised heuristics. (best viewed in color)\\n44'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 44}, page_content='Preprint. A version of this work is under review.\\nN.2 Performance When Knowing The Dataset Name\\nTo further investigate potential data contamination, we conducted an experiment inspired\\nby the methodology described in Golchin & Surdeanu (2024). This involves assessing model\\nperformance under two conditions: with and without explicit knowledge of the dataset\\nbeing evaluated. Specifically, we modify the prompt shown in Figure 9 to mention the name\\nof the dataset (e.g., Friedman #1 ,Friedman #2 ,Friedman #3 ), as detailed in Figure 42. This\\napproach allows us to discern the impact of dataset awareness on the model’s predictive\\naccuracy, providing insights into the extent of potential contamination in the training data.\\nWe present the comparative results in Table 9, which shows the average absolute error under\\nconditions of dataset awareness versus unawareness. Notably, the mean absolute errors ( ↓)\\nremain closely matched across all scenarios. To statistically substantiate these observations,\\nwe performed paired t-tests for each dataset comparison. Given the multiplicity of tests\\nperformed, it became imperative to apply an adjustment for multiple comparisons to our\\np-values Dunn (1961); Benjamini & Hochberg (1995). Following this adjustment, none of the\\np-values remained below the (typically used) 0.05 threshold, suggesting that the knowledge\\nof the dataset name does not significantly affect model performance. Prior to adjustment, in\\ntwo cases the resulting p-value was under 0.05: GPT-4 on Friedman #3 (p-value 0.045) and\\nClaude 3 Sonnet on Friedman #3 (p-value 0.018). Note, however, that only in the case of\\nGPT-4 was the performance corresponding to the Dataset Aware setting better. This analysis\\nindicates that, within the bounds of statistical significance, there is no substantial evidence\\nto suggest that the performance of the models is influenced by explicit knowledge of the\\ndataset name, something which has been linked to contamination (Golchin & Surdeanu,\\n2024).20\\nThe task is to provide your best estimate for ”Output” (Friedman\\n#1). Please provide that and only that, without any additional text.\\nFeature 0: . . .\\nFigure 42: The prompt we use to further investigate whether the large language models\\nhave seen instances of the datasets we tested them on.\\n20We investigated why the performance of Claude 3 Sonnet degraded on Friedman #2 when given\\nthe dataset name. We found that it is (mostly) because of an outlier: the absolute difference between\\nthe model’s prediction and expected output is >300.\\n45'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 45}, page_content='Preprint. A version of this work is under review.\\nFriedman #1 Friedman #2 Friedman #3\\nMAE Dataset Aware 2.820 ±2.332 39.086 ±64.787 0.089 ±0.143\\nMAE Dataset Unaware 2.798 ±2.242 32.787 ±66.165 0.102 ±0.156\\n(a) GPT-4\\nFriedman #1 Friedman #2 Friedman #3\\nMAE Dataset Aware 1.933 ±1.401 5.435 ±7.460 0.074 ±0.163\\nMAE Dataset Unaware 2.002 ±1.470 6.374 ±10.440 0.070 ±0.148\\n(b) Claude 3 Opus\\nFriedman #1 Friedman #2 Friedman #3\\nMAE Dataset Aware 2.118 ±1.791 11.332 ±33.119 0.095 ±0.175\\nMAE Dataset Unaware 2.192 ±1.748 5.375 ±5.940 0.088 ±0.166\\n(c) Claude 3 Sonnet\\nTable 9: Comparison of Mean Absolute Error (MAE ↓) for Models With and Without Dataset\\nName Awareness.\\nO Does The Performance Plateau?\\nIn the following, we investigate whether the performance of LLMs continues to improve as\\nthe number of in-context exemplars increases. Because this experiment is dependent on the\\ncontext size of the LLM, this imposes additional constraints on which LLMs we can use. To\\nthis end, we experimented with the following dataset sizes: {20, 50, 60, 70, 80, 90, 100, 150,\\n200, 250, 300, 400, 500 }.\\nWe experiment with ChatGPT and GPT-4. For GPT-4, we found that the performance keeps\\nimproving with the number of in-context exemplars at least until 500. We present our results\\nin Figures 43, 44, 45, 46, 47. We repeated each experiment 20 times for each dataset size. We\\nreport the mean and 95% confidence.\\nAggregating the results, we observed that GPT-4 performed better than Random Forest in\\n92% of the cases, better than Gradient Boosting in 51% of the cases, and better than Linear\\nRegression with Polynomial Features ( Linear Regression + Poly ) in 40% of the cases,\\nacross all 5 datasets and all dataset sizes.\\nFor example, from Figure 44 we can observe that while Linear Regression + Poly per-\\nforms much better than GPT-4 in small data regimes, this performance gap decreases as the\\nnumber of examples increases, suggesting that the model is indeed capable of leveraging a\\nlarger number of examples.\\n46'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 46}, page_content='Preprint. A version of this work is under review.\\nFigure 43: Performance comparison between GPT-4 and three traditional supervised models\\non the Friedman #1 dataset. The performance of GPT-4 remains good, outperforming\\nRandom Forest. (best viewed in color)\\nFigure 44: Performance comparison between GPT-4 and three traditional supervised models\\non the Friedman #2 dataset. The performance of GPT-4 improves with the number of\\nexamples given, approaching that of Linear Regression + Poly. (best viewed in color)\\n47'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 47}, page_content='Preprint. A version of this work is under review.\\nFigure 45: Performance comparison between GPT-4 and three traditional supervised models\\non the Friedman #3 dataset. (best viewed in color)\\nFigure 46: Performance comparison between GPT-4 and three traditional supervised models\\non the Original #1 dataset. The performance of GPT-4 increases with the number of examples\\ngiven. (best viewed in color)\\n48'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 48}, page_content='Preprint. A version of this work is under review.\\nFigure 47: Performance comparison between GPT-4 and three traditional supervised models\\non the Original #2 dataset. The performance of GPT-4 is no worse to that of Random Forest\\n(or, to a lesser extent, to that of Gradient Boosting) as the number of examples increases. (best\\nviewed in color)\\nP Beyond Transformer-Based LLMs\\nIn our study, we initially focused on transformer-based large language models (LLMs).\\nTo broaden our scope, we explore the capabilities of non-transformer LLMs, including a\\nRWKV-based 14B LLM (Peng et al., 2023) and with StripedHyena Poli et al. (2023a;b), a\\n7B LLM. The performance rankings for these models, along with the transformer-based\\nMistral 7B for comparison, are illustrated in the heatmap provided in Figure 48. We tried\\nrunning Falcon 7B as well, but it produced invalid outputs for almost all examples and all\\ndatasets, therefore we skip it. StripedHyena also encountered difficulties, producing invalid\\noutputs in certain scenarios, such as 98% invalid responses for the Friedman #2 dataset.\\nConsequently, we omitted Friedman #2 andOriginal #2 from its evaluation. However, it\\nis important to highlight that the other models evaluated did not exhibit these issues and\\nwere able to generate valid outputs consistently. We make the following observations.\\nFirst, we remark that performance-wise, RWKV is worse than traditional transformer-based\\nLLMs, although it generally remains better than our unsupervised baselines, with the\\nexception on two linear regression datasets: Regression NI 1/3 andRegression NI 2/3 .\\nNevertheless, we remark that on Original #1 , RWKV outperforms many MLP variants,\\ndespite no gradient updates.\\nSecond, we remark that the performance of Striped Hyena 7B is generally lower than than\\nsome of our unsupervised baselines. We note that there is a notable exception for Original\\n#1. For this dataset, KNN approaches work well, as evident by the good performance\\nobtained by nearest neighbor approaches.\\n49'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\Vinicius Moraes\\\\Google Drive\\\\estudos\\\\data science, machine learning, estatistica\\\\DSA\\\\ia generativa e llms\\\\mod05\\\\87-Cap05\\\\arquivos2\\\\2404.07544v2.pdf', 'page': 49}, page_content='Preprint. A version of this work is under review.\\nFigure 48: The ranks of RWKV V4 14B and (RWKV Architecture) and StripedHyena Nous 7B\\n(Hyena Architecture) compared with traditional supervised method. We included Mistral\\n7B for comparison, as it is a model similar in size. Notably, RWKV’s performance, while\\ngenerally lower than that of transformer-based LLMs, still surpasses our unsupervised\\nbaselines. (best viewed in color)\\n50')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_artigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função do LangChain que cria um índice vetorial (vector store) a partir de uma lista de documentos fornecidos. Esse índice vetorial permite buscas por \n",
    "# similaridade, onde documentos podem ser recuperados com base na similaridade de seus embeddings\n",
    "# texto_artigo é uma lista de objetos Document ou textos que contêm o conteúdo dos documentos a serem armazenados no índice vetorial\n",
    "index = Chroma.from_documents(documents=texto_artigo, embedding=gerador_embeddings, collection_metadata=index_nome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x2354f153fb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo função para fazer a busca por similaridade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_similaridade(query, k = 2):\n",
    "\n",
    "    resultado_similar = index.similarity_search(query, k=k)\n",
    "\n",
    "    return resultado_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o app do assistente pessoal com LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=os.environ['OPENAI_API_KEY'], temperature = .3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm, chain_type = 'stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a função para obter resposta\n",
    "def obter_resposta(query):\n",
    "\n",
    "    # Chama a função 'dsa_busca_similaridade' com a 'query' fornecida e armazena o resultado em 'doc_search'\n",
    "    # Utiliza o método 'similarity_search' do objeto 'index' para buscar os 'k' resultados mais semelhantes à 'query'\n",
    "    similar = busca_similaridade(query)\n",
    "\n",
    "    # Utiliza o objeto 'chain' para executar a função run e processar a 'query' e os documentos encontrados, armazenando a resposta em 'response'\n",
    "    # faz parte de um pipeline de perguntas e respostas c/ o framework LangChain, c/ objetivo de responder a uma pergunta (query) com base em documentos \n",
    "    # fornecidos\n",
    "    resposta = chain.run(input_documents=similar, question=query)\n",
    "\n",
    "    return resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executando o assistente de IA e conversando com PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = \"sobre o que fala o documento? responda em português do Brasil\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " O documento fala sobre modelos de linguagem de grande escala (LLMs) e sua utilização em tarefas de processamento de linguagem natural (PLN). Ele apresenta uma análise dos diferentes LLMs disponíveis, suas características e desempenho em tarefas específicas. Além disso, o documento discute o impacto do pré-treinamento e do contexto no aprendizado desses modelos.\n"
     ]
    }
   ],
   "source": [
    "resposta = obter_resposta(prompt1)\n",
    "print(resposta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
